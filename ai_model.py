#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Willis AI ëª¨ë¸ ì•„í‚¤í…ì²˜ (Neuro-T ê¸°ë°˜)
- ë…¼ë¬¸ ì°¸ê³ : CNN ê¸°ë°˜ ë”¥ëŸ¬ë‹ ëª¨ë¸ (90% ì •í™•ë„ ëª©í‘œ)
"""

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers, models
import numpy as np
from typing import Tuple


class WillisNeuroModel:
    """Willis ì•ˆë©´ ê³„ì¸¡ AI ëª¨ë¸"""
    
    def __init__(self, input_shape: Tuple[int, int] = (468, 2)):
        """
        ì´ˆê¸°í™”
        
        Args:
            input_shape: ì…ë ¥ ëœë“œë§ˆí¬ shape (468ê°œ ì¢Œí‘œ, x/y)
        """
        self.input_shape = input_shape
        self.model = self._build_model()
    
    def _build_model(self) -> keras.Model:
        """
        CNN ëª¨ë¸ êµ¬ì¶• (Neuro-T ìŠ¤íƒ€ì¼)
        
        ì…ë ¥: 468ê°œ ëœë“œë§ˆí¬ ì¢Œí‘œ
        ì¶œë ¥: 3ê°œ í´ë˜ìŠ¤ (í‰ê·  ì´í•˜ / ì •ìƒ / í‰ê·  ì´ìƒ)
        """
        # ì…ë ¥ ë ˆì´ì–´
        inputs = layers.Input(shape=self.input_shape, name='landmarks_input')
        
        # Flatten
        x = layers.Flatten()(inputs)
        
        # Dense layers with Batch Normalization and Dropout
        x = layers.Dense(512, activation='relu', name='dense1')(x)
        x = layers.BatchNormalization()(x)
        x = layers.Dropout(0.3)(x)
        
        x = layers.Dense(256, activation='relu', name='dense2')(x)
        x = layers.BatchNormalization()(x)
        x = layers.Dropout(0.3)(x)
        
        x = layers.Dense(128, activation='relu', name='dense3')(x)
        x = layers.BatchNormalization()(x)
        x = layers.Dropout(0.2)(x)
        
        x = layers.Dense(64, activation='relu', name='dense4')(x)
        x = layers.BatchNormalization()(x)
        
        # ì¶œë ¥ ë ˆì´ì–´ (3-class classification)
        outputs = layers.Dense(3, activation='softmax', name='classification')(x)
        
        # ëª¨ë¸ ìƒì„±
        model = models.Model(inputs=inputs, outputs=outputs, name='willis_neuro_model')
        
        return model
    
    def compile_model(self, learning_rate: float = 0.001):
        """ëª¨ë¸ ì»´íŒŒì¼"""
        self.model.compile(
            optimizer=keras.optimizers.Adam(learning_rate=learning_rate),
            loss='sparse_categorical_crossentropy',
            metrics=['accuracy', 
                    keras.metrics.Precision(name='precision'),
                    keras.metrics.Recall(name='recall')]
        )
    
    def summary(self):
        """ëª¨ë¸ ìš”ì•½"""
        self.model.summary()
    
    def train(self, 
              X_train: np.ndarray, 
              y_train: np.ndarray,
              X_val: np.ndarray = None,
              y_val: np.ndarray = None,
              epochs: int = 50,
              batch_size: int = 32) -> keras.callbacks.History:
        """
        ëª¨ë¸ í•™ìŠµ
        
        Args:
            X_train: í•™ìŠµ ë°ì´í„° (n_samples, 468, 2)
            y_train: í•™ìŠµ ë¼ë²¨ (n_samples,) - 0: í‰ê·  ì´í•˜, 1: ì •ìƒ, 2: í‰ê·  ì´ìƒ
            X_val: ê²€ì¦ ë°ì´í„°
            y_val: ê²€ì¦ ë¼ë²¨
            epochs: ì—í­ ìˆ˜
            batch_size: ë°°ì¹˜ í¬ê¸°
        
        Returns:
            í•™ìŠµ íˆìŠ¤í† ë¦¬
        """
        # ì½œë°± ì„¤ì •
        callbacks = [
            keras.callbacks.EarlyStopping(
                monitor='val_loss' if X_val is not None else 'loss',
                patience=10,
                restore_best_weights=True
            ),
            keras.callbacks.ReduceLROnPlateau(
                monitor='val_loss' if X_val is not None else 'loss',
                factor=0.5,
                patience=5,
                min_lr=1e-6
            ),
            keras.callbacks.ModelCheckpoint(
                'data/training/best_model.keras',
                monitor='val_accuracy' if X_val is not None else 'accuracy',
                save_best_only=True
            )
        ]
        
        # í•™ìŠµ
        validation_data = (X_val, y_val) if X_val is not None else None
        
        history = self.model.fit(
            X_train, y_train,
            validation_data=validation_data,
            epochs=epochs,
            batch_size=batch_size,
            callbacks=callbacks,
            verbose=1
        )
        
        return history
    
    def predict(self, X: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
        """
        ì˜ˆì¸¡
        
        Args:
            X: ì…ë ¥ ë°ì´í„° (n_samples, 468, 2)
        
        Returns:
            (í´ë˜ìŠ¤, í™•ë¥ ) íŠœí”Œ
        """
        probabilities = self.model.predict(X)
        classes = np.argmax(probabilities, axis=1)
        return classes, probabilities
    
    def evaluate(self, X_test: np.ndarray, y_test: np.ndarray):
        """ëª¨ë¸ í‰ê°€"""
        results = self.model.evaluate(X_test, y_test, verbose=0)
        print("\n" + "=" * 60)
        print("ëª¨ë¸ í‰ê°€ ê²°ê³¼")
        print("=" * 60)
        print(f"Loss: {results[0]:.4f}")
        print(f"Accuracy: {results[1]:.4f} ({results[1]*100:.1f}%)")
        print(f"Precision: {results[2]:.4f}")
        print(f"Recall: {results[3]:.4f}")
        print("=" * 60)
        
        # ë…¼ë¬¸ ê¸°ì¤€ ì²´í¬ (90% ëª©í‘œ)
        if results[1] >= 0.90:
            print("âœ… ë…¼ë¬¸ ê¸°ì¤€ ë‹¬ì„±! (90% ì´ìƒ)")
        else:
            print(f"âš ï¸  ë…¼ë¬¸ ê¸°ì¤€ ë¯¸ë‹¬ (ëª©í‘œ: 90%, í˜„ì¬: {results[1]*100:.1f}%)")
    
    def save(self, path: str = "data/training/willis_model.keras"):
        """ëª¨ë¸ ì €ì¥"""
        self.model.save(path)
        print(f"âœ… ëª¨ë¸ ì €ì¥: {path}")
    
    @classmethod
    def load(cls, path: str = "data/training/willis_model.keras"):
        """ëª¨ë¸ ë¡œë“œ"""
        instance = cls()
        instance.model = keras.models.load_model(path)
        print(f"âœ… ëª¨ë¸ ë¡œë“œ: {path}")
        return instance


class FeatureBasedModel:
    """íŠ¹ì§• ê¸°ë°˜ ê°„ë‹¨í•œ ëª¨ë¸ (ëœë“œë§ˆí¬ ëŒ€ì‹  Willis ì¸¡ì •ê°’ ì‚¬ìš©)"""
    
    def __init__(self):
        """ì´ˆê¸°í™”"""
        self.model = self._build_model()
    
    def _build_model(self) -> keras.Model:
        """
        ê°„ë‹¨í•œ MLP ëª¨ë¸
        
        ì…ë ¥: 5ê°œ íŠ¹ì§• (willis_ratio, pupil_to_mouth, nose_to_chin, symmetry, is_frontal)
        ì¶œë ¥: 3ê°œ í´ë˜ìŠ¤
        """
        inputs = layers.Input(shape=(5,), name='features_input')
        
        x = layers.Dense(64, activation='relu')(inputs)
        x = layers.BatchNormalization()(x)
        x = layers.Dropout(0.3)(x)
        
        x = layers.Dense(32, activation='relu')(x)
        x = layers.BatchNormalization()(x)
        x = layers.Dropout(0.2)(x)
        
        x = layers.Dense(16, activation='relu')(x)
        
        outputs = layers.Dense(3, activation='softmax', name='classification')(x)
        
        model = models.Model(inputs=inputs, outputs=outputs, name='willis_feature_model')
        
        return model
    
    def compile_model(self, learning_rate: float = 0.001):
        """ëª¨ë¸ ì»´íŒŒì¼"""
        self.model.compile(
            optimizer=keras.optimizers.Adam(learning_rate=learning_rate),
            loss='sparse_categorical_crossentropy',
            metrics=['accuracy']
        )
    
    def summary(self):
        """ëª¨ë¸ ìš”ì•½"""
        self.model.summary()
    
    def train(self, X_train, y_train, X_val=None, y_val=None, epochs=100, batch_size=16):
        """ëª¨ë¸ í•™ìŠµ"""
        callbacks = [
            keras.callbacks.EarlyStopping(patience=15, restore_best_weights=True),
            keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=7, min_lr=1e-6)
        ]
        
        validation_data = (X_val, y_val) if X_val is not None else None
        
        history = self.model.fit(
            X_train, y_train,
            validation_data=validation_data,
            epochs=epochs,
            batch_size=batch_size,
            callbacks=callbacks,
            verbose=1
        )
        
        return history
    
    def predict(self, X):
        """ì˜ˆì¸¡"""
        probabilities = self.model.predict(X)
        classes = np.argmax(probabilities, axis=1)
        return classes, probabilities
    
    def evaluate(self, X_test, y_test):
        """ëª¨ë¸ í‰ê°€"""
        results = self.model.evaluate(X_test, y_test, verbose=0)
        print(f"\nTest Loss: {results[0]:.4f}")
        print(f"Test Accuracy: {results[1]:.4f} ({results[1]*100:.1f}%)")
    
    def save(self, path: str = "data/training/willis_feature_model.keras"):
        """ëª¨ë¸ ì €ì¥"""
        self.model.save(path)
        print(f"âœ… ëª¨ë¸ ì €ì¥: {path}")
    
    @classmethod
    def load(cls, path: str = "data/training/willis_feature_model.keras"):
        """ëª¨ë¸ ë¡œë“œ"""
        instance = cls()
        instance.model = keras.models.load_model(path)
        print(f"âœ… ëª¨ë¸ ë¡œë“œ: {path}")
        return instance


def demo_training():
    """ë°ëª¨ í•™ìŠµ (ì‘ì€ ìƒ˜í”Œ ë°ì´í„°)"""
    print("=" * 60)
    print("Willis AI ëª¨ë¸ ë°ëª¨")
    print("=" * 60)
    
    # ë”ë¯¸ ë°ì´í„° ìƒì„±
    print("\nğŸ“¦ ë”ë¯¸ ë°ì´í„° ìƒì„± ì¤‘...")
    n_samples = 100
    
    # ëœë“œë§ˆí¬ ë°ì´í„° (468ê°œ ì¢Œí‘œ)
    X_landmarks = np.random.rand(n_samples, 468, 2)
    
    # íŠ¹ì§• ë°ì´í„° (5ê°œ íŠ¹ì§•)
    X_features = np.random.rand(n_samples, 5)
    X_features[:, 0] = np.random.uniform(0.8, 1.2, n_samples)  # willis_ratio
    
    # ë¼ë²¨ (0: í‰ê·  ì´í•˜, 1: ì •ìƒ, 2: í‰ê·  ì´ìƒ)
    y = np.random.randint(0, 3, n_samples)
    
    # ë¶„í• 
    split = int(0.8 * n_samples)
    X_landmarks_train, X_landmarks_val = X_landmarks[:split], X_landmarks[split:]
    X_features_train, X_features_val = X_features[:split], X_features[split:]
    y_train, y_val = y[:split], y[split:]
    
    # 1. Neuro-T ìŠ¤íƒ€ì¼ ëª¨ë¸
    print("\n" + "=" * 60)
    print("1. Neuro-T ìŠ¤íƒ€ì¼ ëª¨ë¸ (ëœë“œë§ˆí¬ ê¸°ë°˜)")
    print("=" * 60)
    neuro_model = WillisNeuroModel()
    neuro_model.summary()
    neuro_model.compile_model()
    print("\nâš ï¸  ì‹¤ì œ ë°ì´í„°ë¡œ í•™ìŠµí•˜ë ¤ë©´ ai_train_prepare.py ì‚¬ìš©")
    
    # 2. íŠ¹ì§• ê¸°ë°˜ ëª¨ë¸
    print("\n" + "=" * 60)
    print("2. íŠ¹ì§• ê¸°ë°˜ ê°„ë‹¨ ëª¨ë¸")
    print("=" * 60)
    feature_model = FeatureBasedModel()
    feature_model.summary()
    feature_model.compile_model()
    
    print("\nğŸš€ íŠ¹ì§• ê¸°ë°˜ ëª¨ë¸ í•™ìŠµ ì‹œì‘...")
    history = feature_model.train(
        X_features_train, y_train,
        X_features_val, y_val,
        epochs=50,
        batch_size=16
    )
    
    print("\nğŸ“Š ìµœì¢… ì„±ëŠ¥:")
    feature_model.evaluate(X_features_val, y_val)


if __name__ == "__main__":
    demo_training()
