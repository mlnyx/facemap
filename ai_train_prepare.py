#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Willis AI ëª¨ë¸ í•™ìŠµ ë°ì´í„° ì¤€ë¹„
- ë…¼ë¬¸ ê¸°ì¤€: 500+ ë¼ë²¨ë§ëœ ì‚¬ì§„ í•„ìš” (90% ì •í™•ë„ ëª©í‘œ)
"""

import cv2
import numpy as np
import json
import os
from pathlib import Path
from typing import List, Dict, Tuple
from dataclasses import dataclass, asdict
from datetime import datetime

from src.core import FaceLandmarker, WillisAnalyzer


@dataclass
class TrainingData:
    """í•™ìŠµ ë°ì´í„° í•­ëª©"""
    image_path: str
    willis_ratio: float
    pupil_to_mouth: float
    nose_to_chin: float
    face_symmetry: float
    classification: str  # ì •ìƒ, í‰ê·  ì´í•˜, í‰ê·  ì´ìƒ
    is_frontal: bool  # ì •ë©´ ì—¬ë¶€
    landmarks: List[Tuple[float, float]]  # 468ê°œ ëœë“œë§ˆí¬
    image_width: int
    image_height: int
    timestamp: str


class TrainingDataCollector:
    """í•™ìŠµ ë°ì´í„° ìˆ˜ì§‘ê¸°"""
    
    def __init__(self, output_dir: str = "data/training"):
        """ì´ˆê¸°í™”"""
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        self.images_dir = self.output_dir / "images"
        self.labels_dir = self.output_dir / "labels"
        self.images_dir.mkdir(exist_ok=True)
        self.labels_dir.mkdir(exist_ok=True)
        
        self.landmarker = FaceLandmarker()
        self.analyzer = WillisAnalyzer()
        
        self.dataset = []
        self.load_existing_dataset()
    
    def load_existing_dataset(self):
        """ê¸°ì¡´ ë°ì´í„°ì…‹ ë¡œë“œ"""
        dataset_path = self.output_dir / "dataset.json"
        if dataset_path.exists():
            with open(dataset_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
                self.dataset = data.get('items', [])
            print(f"âœ“ ê¸°ì¡´ ë°ì´í„°ì…‹ ë¡œë“œ: {len(self.dataset)}ê±´")
        else:
            print("âœ“ ìƒˆ ë°ì´í„°ì…‹ ì‹œì‘")
    
    def save_dataset(self):
        """ë°ì´í„°ì…‹ ì €ì¥"""
        dataset_path = self.output_dir / "dataset.json"
        data = {
            'version': '1.0',
            'created_at': datetime.now().isoformat(),
            'total_count': len(self.dataset),
            'items': self.dataset
        }
        with open(dataset_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        print(f"âœ… ë°ì´í„°ì…‹ ì €ì¥: {len(self.dataset)}ê±´")
    
    def process_image(self, image_path: str, copy_image: bool = True) -> TrainingData:
        """ì´ë¯¸ì§€ ì²˜ë¦¬ ë° ë°ì´í„° ì¶”ì¶œ"""
        image = cv2.imread(image_path)
        if image is None:
            raise ValueError(f"ì´ë¯¸ì§€ë¥¼ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {image_path}")
        
        h, w = image.shape[:2]
        landmarks = self.landmarker.get_landmarks(image)
        
        if not landmarks:
            raise ValueError(f"ì–¼êµ´ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {image_path}")
        
        result = self.analyzer.analyze(landmarks, w, h)
        
        # ëœë“œë§ˆí¬ë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
        landmarks_list = [(lm.x * w, lm.y * h) for lm in landmarks]
        
        # ì´ë¯¸ì§€ ë³µì‚¬
        if copy_image:
            filename = f"{datetime.now().strftime('%Y%m%d_%H%M%S')}_{os.path.basename(image_path)}"
            new_image_path = self.images_dir / filename
            cv2.imwrite(str(new_image_path), image)
            rel_image_path = str(new_image_path.relative_to(self.output_dir))
        else:
            rel_image_path = image_path
        
        return TrainingData(
            image_path=rel_image_path,
            willis_ratio=result.ratio,
            pupil_to_mouth=result.pupil_to_mouth_distance,
            nose_to_chin=result.nose_to_chin_distance,
            face_symmetry=result.face_symmetry,
            classification=result.classification.value,
            is_frontal=result.face_symmetry >= 0.85,
            landmarks=landmarks_list,
            image_width=w,
            image_height=h,
            timestamp=datetime.now().isoformat()
        )
    
    def add_image(self, image_path: str, copy_image: bool = True):
        """ì´ë¯¸ì§€ ì¶”ê°€"""
        try:
            data = self.process_image(image_path, copy_image)
            self.dataset.append(asdict(data))
            print(f"âœ“ ì¶”ê°€: {image_path} (ë¹„ìœ¨: {data.willis_ratio:.3f}, {data.classification})")
        except Exception as e:
            print(f"âœ— ì‹¤íŒ¨: {image_path} - {e}")
    
    def batch_add(self, image_paths: List[str], copy_images: bool = True):
        """ë°°ì¹˜ ì¶”ê°€"""
        print(f"\nğŸ“¦ {len(image_paths)}ê°œ ì´ë¯¸ì§€ ì²˜ë¦¬ ì¤‘...\n")
        for path in image_paths:
            self.add_image(path, copy_images)
        self.save_dataset()
    
    def get_statistics(self) -> Dict:
        """ë°ì´í„°ì…‹ í†µê³„"""
        if not self.dataset:
            return {}
        
        classifications = [item['classification'] for item in self.dataset]
        ratios = [item['willis_ratio'] for item in self.dataset]
        frontals = [item['is_frontal'] for item in self.dataset]
        
        return {
            'total_count': len(self.dataset),
            'normal_count': classifications.count('ì •ìƒ'),
            'below_count': classifications.count('í‰ê·  ì´í•˜'),
            'above_count': classifications.count('í‰ê·  ì´ìƒ'),
            'frontal_count': sum(frontals),
            'non_frontal_count': len(frontals) - sum(frontals),
            'avg_ratio': np.mean(ratios),
            'min_ratio': np.min(ratios),
            'max_ratio': np.max(ratios),
            'std_ratio': np.std(ratios)
        }
    
    def print_statistics(self):
        """í†µê³„ ì¶œë ¥"""
        stats = self.get_statistics()
        if not stats:
            print("ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")
            return
        
        print("\n" + "=" * 60)
        print("í•™ìŠµ ë°ì´í„° í†µê³„")
        print("=" * 60)
        print(f"ì´ ë°ì´í„°: {stats['total_count']}ê±´")
        print(f"  - ì •ìƒ: {stats['normal_count']}ê±´ ({stats['normal_count']/stats['total_count']*100:.1f}%)")
        print(f"  - í‰ê·  ì´í•˜: {stats['below_count']}ê±´ ({stats['below_count']/stats['total_count']*100:.1f}%)")
        print(f"  - í‰ê·  ì´ìƒ: {stats['above_count']}ê±´ ({stats['above_count']/stats['total_count']*100:.1f}%)")
        print()
        print(f"ì •ë©´ ì‚¬ì§„: {stats['frontal_count']}ê±´ ({stats['frontal_count']/stats['total_count']*100:.1f}%)")
        print(f"ì¸¡ë©´/ë¹„ì •ë©´: {stats['non_frontal_count']}ê±´")
        print()
        print(f"Willis ë¹„ìœ¨ í‰ê· : {stats['avg_ratio']:.3f} Â± {stats['std_ratio']:.3f}")
        print(f"  - ìµœì†Œê°’: {stats['min_ratio']:.3f}")
        print(f"  - ìµœëŒ€ê°’: {stats['max_ratio']:.3f}")
        print("=" * 60)
        
        # ë…¼ë¬¸ ê¸°ì¤€ ì²´í¬
        target = 500
        remaining = max(0, target - stats['total_count'])
        print(f"\nğŸ“Š ë…¼ë¬¸ ê¸°ì¤€ (500ê±´) ë‹¬ì„±ë¥ : {stats['total_count']/target*100:.1f}%")
        if remaining > 0:
            print(f"   âš ï¸  {remaining}ê±´ ë” í•„ìš”")
        else:
            print(f"   âœ… ëª©í‘œ ë‹¬ì„±!")
    
    def export_for_training(self):
        """í•™ìŠµìš© ë°ì´í„° ë‚´ë³´ë‚´ê¸° (CSV, NumPy)"""
        if not self.dataset:
            print("ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")
            return
        
        # CSV ë‚´ë³´ë‚´ê¸°
        csv_path = self.output_dir / "training_data.csv"
        with open(csv_path, 'w', encoding='utf-8') as f:
            f.write("image_path,willis_ratio,classification,pupil_to_mouth,nose_to_chin,symmetry,is_frontal\n")
            for item in self.dataset:
                f.write(f"{item['image_path']},{item['willis_ratio']},{item['classification']},"
                       f"{item['pupil_to_mouth']},{item['nose_to_chin']},{item['face_symmetry']},{item['is_frontal']}\n")
        print(f"âœ… CSV ë‚´ë³´ë‚´ê¸°: {csv_path}")
        
        # NumPy ë°°ì—´ ë‚´ë³´ë‚´ê¸°
        features = np.array([[
            item['willis_ratio'],
            item['pupil_to_mouth'],
            item['nose_to_chin'],
            item['face_symmetry'],
            1.0 if item['is_frontal'] else 0.0
        ] for item in self.dataset])
        
        labels = np.array([
            0 if item['classification'] == 'í‰ê·  ì´í•˜' 
            else 1 if item['classification'] == 'ì •ìƒ'
            else 2
            for item in self.dataset
        ])
        
        np.save(self.output_dir / "features.npy", features)
        np.save(self.output_dir / "labels.npy", labels)
        print(f"âœ… NumPy ë°°ì—´ ë‚´ë³´ë‚´ê¸°: features.npy, labels.npy")
        print(f"   Features shape: {features.shape}")
        print(f"   Labels shape: {labels.shape}")


def main():
    """ë©”ì¸ ì‹¤í–‰"""
    import sys
    import glob
    
    collector = TrainingDataCollector()
    
    if len(sys.argv) < 2:
        print("\n" + "=" * 60)
        print("Willis AI í•™ìŠµ ë°ì´í„° ì¤€ë¹„")
        print("=" * 60)
        print("\nì‚¬ìš©ë²•:")
        print("  python ai_train_prepare.py <command> [args]")
        print("\nëª…ë ¹ì–´:")
        print("  add <image_path>          - ë‹¨ì¼ ì´ë¯¸ì§€ ì¶”ê°€")
        print("  batch <pattern>           - ì—¬ëŸ¬ ì´ë¯¸ì§€ ì¼ê´„ ì¶”ê°€")
        print("  stats                     - í†µê³„ ë³´ê¸°")
        print("  export                    - í•™ìŠµìš© ë°ì´í„° ë‚´ë³´ë‚´ê¸°")
        print("\nì˜ˆì‹œ:")
        print("  python ai_train_prepare.py add data/input/ì •ë©´1.jpeg")
        print("  python ai_train_prepare.py batch 'data/input/*.jpeg'")
        print("  python ai_train_prepare.py stats")
        print("  python ai_train_prepare.py export")
        sys.exit(0)
    
    command = sys.argv[1]
    
    if command == "add":
        if len(sys.argv) < 3:
            print("âŒ ì´ë¯¸ì§€ ê²½ë¡œë¥¼ ì§€ì •í•˜ì„¸ìš”")
            sys.exit(1)
        collector.add_image(sys.argv[2])
        collector.save_dataset()
        collector.print_statistics()
    
    elif command == "batch":
        if len(sys.argv) < 3:
            print("âŒ ì´ë¯¸ì§€ íŒ¨í„´ì„ ì§€ì •í•˜ì„¸ìš”")
            sys.exit(1)
        pattern = sys.argv[2]
        image_paths = glob.glob(pattern)
        if not image_paths:
            print(f"âŒ ë§¤ì¹­ë˜ëŠ” íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {pattern}")
            sys.exit(1)
        collector.batch_add(image_paths)
        collector.print_statistics()
    
    elif command == "stats":
        collector.print_statistics()
    
    elif command == "export":
        collector.export_for_training()
        collector.print_statistics()
    
    else:
        print(f"âŒ ì•Œ ìˆ˜ ì—†ëŠ” ëª…ë ¹ì–´: {command}")
        sys.exit(1)


if __name__ == "__main__":
    main()
