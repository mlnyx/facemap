  
저작자표시-비영리-변경금지 2.0 대한민국 
이용자는 아래의 조건을 따르는 경우에 한하여 자유롭게 
l이 저작물을 복제, 배포, 전송, 전시, 공연 및 방송할 수 있습니다.  
다음과 같은 조건을 따라야 합니다: 
l귀하는, 이 저작물의 재이용이나 배포의 경우, 이 저작물에 적용된 이용허락조건
을 명확하게 나타내어야 합니다.  
l저작권자로부터 별도의 허가를 받으면 이러한 조건들은 적용되지 않습니다.  
저작권법에 따른 이용자의 권리는 위의 내용에 의하여 영향을 받지 않습니다. 
이것은 이용허락규약(Legal Code)을 이해하기 쉽게 요약한 것입니다.  
Disclaimer  
  
  
저작자표시. 귀하는 원저작자를 표시하여야 합니다. 
비영리. 귀하는 이 저작물을 영리 목적으로 이용할 수 없습니다. 
변경금지. 귀하는 이 저작물을 개작, 변형 또는 가공할 수 없습니다. 

- 1 - 
  
치의학  박사학위 논문  
 
딥러닝  기반 Neuro -T 모델을  
이용한  Willis 안면 계측법의   
안모 비율 분류 정확도   
및 교육적  효과 평가  
 
전남대학교  대학원  
치 의 학 과 
 
김   수   헌 
 
 
 
 
 
2025년 8월 

 
 목 차 
 
국문 초록  ································ ································ ······························  1 
 
I. 서론  ································ ································ ································ ·· 5 
 
II. 재료 및 방법  ································ ································ ·····················  10 
1. 데이터  수집 및 라벨링  방법 ································ ································ · 10 
2. 정면 안면 디지털  사진, 측면 안면 디지털  사진, 측방두부규격방사선사진에서  
이미지  전처리  방법 ································ ································ ·················  11 
3. 자동화된  DCNN  프로그램  ································ ································ ····· 13 
4. Willis 안면  계측법을  이용한  안모  비율  측정  및 그룹  분류 ·······················  14 
5. Neuro -T 프로그램의  Willis 안면  계측법  분류  정확도  및 영향  요인  평가 ····· 17 
가. 표준 사진과  크롭 사진의  성능 비교 및 Grad -CAM 분석 ·····················  20 
나. 사진 유형별  성능 비교 및 Grad -CAM 기반 오류 원인 분석 ················  20 
다. 데이터  증강 기법이  분류 정확도에  미치는  영향 평가 ························  20 
라. 학습 데이터  양이 분류 정확도에  미치는  영향 평가 ···························  21 
6. 딥러닝  지원이  학생과  전공의의  분류 정확도 에 미치는  영향 평가 ·············  22 
 
III. 결과 ································ ································ ································ ········  27 
1. Neuro -T 프로그램의  Willis 안면 계측법  분류 정확도  평가 ···························  27 
 
 가. 표준 사진과  크롭 사진의  성능 비교 및 Grad -CAM 분석 결과 ·············  29 
나. 사진 유형별  성능 비교 및 Grad -CAM 기반 오류 원인 분석 결과 ·········  32 
다. 데이터  증강 기법이  분류 정확도에  미치는  영향 ································  34 
라. 학습 데이터  양이 분류 정확도에  미치는  영향 ································ ··· 36 
2. 딥러닝  지원이  학생과  전공의의  분류 정확도에  미치는  영향 ·····················  38 
가. 딥러닝의  지원 전 분류 정확도  ································ ························  38 
나. 딥러닝의  지원 후 분류 정확도  ································ ························  38 
 
IV. 고찰 ································ ································ ································ ······· 48 
 
V. 결론································ ································ ································ ·········  60 
 
참조문헌  ································ ································ ································ ········  61 
 
Abstract (영문초록 ) ································ ································ ··························  72 
 
 
 
 
 
 
- 1 - 
 딥러닝  기반 Neuro -T 모델을  이용한   
Willis 안면 계측법의  안모 비율 분류 정확도  
및 교육적  효과 평가 
 
김 수 헌 
 
전남대학교대학원  치의학과 
(지도교수  : 윤귀덕 ) 
 
 
(국문초록 ) 
목적 : 본 연구의  목적은  Willis 안면 계측법 에 따른 안모 비율이 정상 범위에  해
당하는지  또는 평균 이하인지  분류하는  딥러닝  모델(Neuro -T)의 성능을  평가하는  
것이다 . 이를 위해 정면 안면 디지털  사진, 측면 안면 디지털  사진, 측방두부 규
격방사선사진을  각각 사용했을  때의 분류 성능을  비교하고 , 각 사진의  표준화된  
사진과 주요 안면 부위를  포함하여  자른 사진 간의 분류 성능을  비교하였다 . 또
한, 대비, 밝기, 색조, 채도 조절, 좌우반전  및 회전을 통한 증강 기법을  적용했
을 때 분류 성능에  미치는  영향을  분석하고 , 딥러닝  프로그램의  최종 정확도에  
도달하는  데 필요한  최적의  사진 개수를  규명하는  것이다 . 마지막으로 , 딥러닝  
프로그램의  결과를  제공하면  치과대학  학생과  전공의의  Willis 안면 계측법의  안
- 2 - 
 모 비율 분류 정확도가  유의하게  증가하는지  평가하는  것이다 . 
 
재료 및 방법 : 전남대학교  치과병원에서  촬영한  정면 안면 디지털  사진, 측면 
안면 디지털  사진, 측방두부 규격방사선사진 을 각각 1,808장, 1,602장, 1,086장 수집
하였다 . 정면 안면 디지털  사진, 측면 안면 디지털  사진, 측방두부 규격방사선사
진을 해부학적  기준에  따라 회전하고  불필요한  배경을  잘라 표준화하였다 . 표준 
사진과  주요 안면 부위를  포함하여  자른 크롭 사진을  Willis의 안면 계측 방법의  
안모 비율에  따라 정상(평균 이상)과 평균 이하 두 그룹으로  분류한  후, Neuro -T
로 훈련시켜  분류 정확도를  평가하였다 . 또한 크롭 정모사진에서  대비, 밝기, 색
조, 채도 조절, 좌우반전  및 2° 회전 증강 기법을  적용하여  새로운  이미지를  생
성하였으며 , 이렇게  생성된  증강 데이터를  추가하여  분류 모델을  학습시키고 , 증
강 전후의  분류 정확도를  평가하였다 . 그리고 , 딥러닝  프로그램의  최종 정확도에  
도달하기  위한 최적의  사진 개수를  찾기 위해 크롭 정모사진에서  그룹당  사진 
개수를  100장씩 700장까지  증가시키며  딥러닝  프로그램의  정확도를  평가하였다 . 
마지막으로 , Neuro -T의 결과를  제공하기  전후로  치과대학  학생 31명과 전공의  30
명에게  Willis의 안면 계측 방법의  안모 비율을  2개 그룹으로  분류하는  설문조사
를 실시하여  정확도  변화를  비교하였다 . 
  
결과 : 정면 안면 디지털  사진, 측면 안면 디지털  사진, 측방두부 규격방사선사진
의 표준 사진에서 Willis의 안면 계측 방법의  안모 비율을  Neuro -T로 분류한  결과 
정확도는  각각 84.66%, 79.33%, 6 3.46%로, 정면 안면 디지털  사진에서  가장 높은 
분류 정확도를  보였으며  측면 안면 디지털  사진과  측방두부 규격방사선사진  순으
- 3 - 
 로 정확도가  낮아졌다 . 정면 안면 디지털  사진, 측면 안면 디지털  사진, 측방두
부규격방사선사진 의 크롭 사진에서도  동일한  경향을 보였으며 , 분류 정확도는  각
각 90.00%, 84 .00%, 73.07% 였다. 사진 유형별 로 표준 사진과  크롭 사진의  분류 정
확도를  비교한  결과, 모든 사진의  유형에서  크롭 사진에서의  정확도가  표준 사진
에서보다  높았다 . 크롭된  정면 안면 디지털  사진을  사용한  경우, 데이터  증강을  
적용한  모델보다  증강 없이 원본 이미지로만  학습한  모델이  더 높은 분류 정확
도를 보였다 . 또한, 크롭된  정면 안면 디지털  사진의 분류 정확도는  그룹당  사진 
수가 500장일 때 최종 정확도에  수렴하였다 . Neuro -T의 분류 결과를  제공한  후, 
치과대학  학생과 전공의의  분류 정확도는  정면 안면 디지털  사진, 측방두부규격
방사선사진에서  유의하게  상승하였다 (p<0.05).  
  
결론 : 본 연구에서  Neuro -T를 사용하여  Willis 안면 계측법의  안모 비율을  분류
한 결과, 정면 안면 디지털  사진을  사용할  때 가장 높은 성능을  보였다 . 모든 유
형의 사진에서  크롭 사진을  사용할  때 모델의  성능이  더욱 향상되었다 . 크롭된  
정면 안면 디지털  사진을  사용한  경우, 데이터  증강을  적용한  모델보다  증강 없
이 원본 이미지만으로  훈련한  모델에서  더 높은 성능을  보였다 . 증강 없이 원본 
이미지만으로  훈련한  모델에서  더 높은 성능을  보였다 . 또한, 크롭된  정면 안면 
디지털  사진을  분류하는  Neuro -T 모델에서  Willis  안면 계측법의 안모 비율을  정
확하게  분류하기  위한 최적의  사진 개수는  그룹당  500장이었다 . Neuro -T 프로그
램의 도움을  받은 후 치과대학  학생과  전공의의  Willis 안면 계측법의  안모 비율 
분류 정확도는  유의하게  증가하였고 , 이로써  Neuro -T 프로그램 의 Willis 안면 계
측법의  비율 분류의  임상적  활용 가능성을  보여주었으며 , 교육적  도구로서의  유
- 4 - 
 용성도  확인되었다 . 
 
- 5 - 
 I. 서 론  
 
 교합 수직 고경(vertical dimension of occlusion, VDO )은 최대감합위에서  선택된  두 
해부학적  위치를  연결한  수직 거리를  의미하며 (1), 이 두 위치는  코끝과  턱끝이  
주로 선택된다 . 병적인  치아 마모, 다수 치아 상실 등으로  인해 수직 고경이 
감소되는  문제는  임상적으로  빈번히  관찰되는  문제로 (2),(3), 이는 환자의  심미성 , 
기능에  중대한  영향을  미친다 . 수직 고경이  감소되면  하안모의  높이가  짧아지고 , 
입술과  볼이 늘어지며  얼굴이  주름지고  노화된  인상을  주게 된다. 또한, 
구각구순염 (angular cheilitis )과 같은 구강 주위 피부 질환이  동반될  수 있다 (4). 
기능적으로는  저작 효율 저하, 발음 장애, 교합 불안정 , 그리고  측두하악관절에  
과하중이  가해져  근신경계  이상, 측두하악장애가  발생할  수 있다 (5). 그리고  수직 
고경이  감소되어  보철 수복 공간이  부족해질  수 있으며  이 때 수직 고경의  변경 
없이 보철 수복을  진행할  경우 보철물의  파절, 탈락, 교합간섭  등이 발생할  수 
있다 (6). 
따라서  이러한  경우 수직 고경의  상실 여부를  평가하고  수직고경을  적절히  
변경하기  위해 전악 보철 수복을  고려할  수 있으며 (7),(8),(9), 이를 위해서는  치료 
전 정확한  진단 과정이  필수적이다 . 수직 고경의  감소를  진단하는  여러 가지 
방법이  소개되어  왔으며 , 그 방법으로  안정위  수직 고경과  교합 수직 고경 간의 
차이인  자유공극 (freeway space )를 이용하는  방법 (10)이나 동공(pupil)에서 
구열(rima oris )까지의  거리와  비저부 (base of the nose )부터 턱끝(chin)까지의  거리가  
같음을  이용하는  Willis 안면 계측법 (11)이 있다. Willis의 방법을  이용하면  
동공에서  구열까지의  거리보다  비저부부터  턱끝까지의  거리가  짧게 측정될  경우 
- 6 - 
 수직 고경이  감소했을  가능성이  있다고  유추할  수 있다. 또한 상하악  순측 전정 
간 거리를  이용하는  방법 (12),(13), 발음 (14)이나 연하 (15)를 이용하는  방법, 
환자의  주관적  평가를  이용하는  방법 (16),(17) 등이 있다. 하지만  자유공극을  
측정한  다양한  연구들은  그 측정 방법에  다소 일관성이  부족하여  평균값이  넓게 
분포하며 (18) 동일한  환자에서도  측정 시마다  달라질  수 있다 (6)는 단점이  있다. 
전정 간 거리를  이용하는  방법의  경우 인상채득  시마다  전정의  위치와  형태가  
변할 수 있으며 (19), 그로 인해 전정 간 거리 측정의  일관성이  떨어질  수 있다. 
발음, 연하를  이용하는  방법은  환자 개개인의  협조도나  습관에  따라 결과가  
달라질  수 있다는  단점이  있다 (20).  
정량적  안모 계측법은  비교적  객관적인  지표로  간주되지만 , 주로 연조직상의  
기준점을  사용하기 때문에  측정 시마다  연조직에  가해지는  압력에 따라 
측정값의  차이가  발생하기  쉽다 (21). 때문에  기구에  의한 연조직  압박 없이 (22) 
표준화된  사진으로부터  인체 계측값을  기록하는  연구들이  보고되었으며 , 
사진측량법 (photogrammetry )은 신뢰할  만한 방법으로  평가된다 (23),(24),(25),(26), 
(27). 따라서, 표준화된  얼굴 사진을  기반으로  딥러닝  프로그램을  이용한  Willis 
안면 계측법은  기존 수기 계측에서  발생하는  연조직  압박, 관찰자  간 변동성  등 
한계를  극복하고 , 동공에서  구열까지의  거리와  비저부부터  턱끝까지의  거리의  
비율을  객관적이고  일관성  있게 분류할 수 있을 것으로  기대된다 (28). 이를 위해 
본 연구에서는  정면, 측면 안면 디지털  사진을  이용하였다 . 또한, 연조직  
랜드마크  식별 시 경조직  정보가  보완적  역할을  할 수 있는지 , 이미지의  유형에  
따라 딥러닝의  성능에  어떤 차이가  나타나는지를  확인하고자   
측방두부규격방사선사진도  함께 이용하였다 . 
- 7 - 
 최근 인공지능 (artificial intelligence, AI )과 딥러닝 (deep learning ) 기술의  발전은  
의학 및 치의학  분야에서  진단의  정확성과  효율성을  크게 높이고  
있다 (29),(30),(31). 특히 딥러닝 의 한 분야인  심층 합성곱  신경망 (deep convolu tional 
neural n etworks , DCNN )은 여러 층의 합성곱  계층을  포함하는  인공 신경망  구조로 , 
주로 이미지  인식, 분류에 특화되어  있다 (32, 33). DCNN 은 복잡한  영상 
데이터에서  미세한  패턴을  자동으로  추출하고  학습할  수 있어 의료 및 치과 
영상 진단에서  활발히  연구되고  있다 (34),(35). DCNN 을 이용해  얼굴의  특징으로  
질환의  여부를  예측하여  분류하거나 (36),(37) 미용적  안면 형태를  분류하는  
연구들이  있으나 (38),(39), 수직 고경의  평가를  위해 안면 비율을  기준으로  
집단을  분류하여  학습하는  연구는  아직 없다.  
 딥러닝  기반 알고리즘을  제작하고  미세조정하는  것은 매우 어려운  작업으로  
상당한  시간이  소요될  수 있으며 , 복잡한  코드 제작 없이 최적의  신경망  구조와  
매개변수 를 빠르게  탐색할  수 있는 자동화된  딥러닝  기술이  개발되었다 (40). 
Neuro -T version 3.1.0. (Neurocle Inc., Seoul, Korea) 는 자동화된  DCNN  아키텍처로 , 
자동 프로그램  선택 및 매개변수  최적화  등을 제공하여  비전문가인  사용자도  
쉽게 사용할  수 있으며  미세 조정에  필요한  시간을  감소시킬  수 있는 장점이  
있다 (35). 이러한  특성으로  인해 Neuro -T는 비전문가인  사용자도  쉽게 사용할  수 
있으며 , 다른 DCNN 보다 성능도  좋다는  연구 결과가  있다 (35),(41).  
 대부분의  의료 영상 데이터는  샘플 수가 제한적인  경우가  많으며 , 이를 
보완하기  위한 방법으로  데이터  증강(data augmentation )이 활용될  수 있다 (42). 
데이터  증강은  기존 학습 데이터를  기반으로  추가적인  학습 샘플을  생성하거나 , 
다양한  기법을  통해 새로운  데이터를  만들어내는  방식으로  모델의  일반화  
- 8 - 
 성능을  향상시키는  데 기여한다 (43). 대표적인  데이터  증강 기법으로는  좌우 
반전 (flip), 회전 (rotation) , 자르기 (cropping) , 노이즈  삽입 (noise injection) , 색상 
변화 (color jitte r), 임의 삭제 (random erasing) , 이미지  혼합 (mix up) , 생성형  모델 
기반 증강 (generative adversarial networks, GANs)  등이 있다 (42),(44). 다양한  
연구에서  데이터  증강이  딥러닝  기반의  이미지  분류 성능을  유의하게  
향상시킨다고  보고하고  있다 (45),(46),(47). 하지만  수직 고경의  평가를  위해 안면 
비율을  분류하는  모델의  성능을  데이터  증강으로  향상시킨다는  연구는  아직 
없다. 
 데이터  샘플 수가 너무 적으면  DCNN 이 정확하게  훈련되지  않고 편향적인  
결과가  도출될  수 있고, 데이터  샘플 수가 너무 많으면  훈련시간이  길어지고  
데이터  수집에  많은 시간과  비용이  발생한다 (48). 따라서 , 효율적인  DCNN  
훈련을  위해서는  일반화  성능이  수렴하는  최적의  샘플 크기를  설정하는  것 이 
중요하다 (49). DCNN 을 효율적으로  훈련시키는  최적의  샘플 크기에  대한 다른 
분야의  연구 (50)가 있지만 , DCNN 이 안면 비율을  정확히  분류할  수 있는 최적의  
사진 개수에  대한 연구는  아직 없다. 
 최근 딥러닝  기술이  의료 영상이나  임상 사진의  판독 및 분석에 서 보조적  
수단으로  사용될  경우 진단의  일관성과  효율성이  향상되는  것으로  보고되고  
있다 (51),(52). 예를 들어, 최근 연구에서는  흉부 방사선사진  판독에  딥러닝  
시스템이  보조로  제공되었을  때 딥러닝의  보조 없이 판독할  때보다  의사들의  
이상 소견 검출 정확도가  증가되고  판독 속도가  단축된  것으로  보고되었다 (51). 
그러나  치과대학  학생과  전공의가  수직 고경의  평가를  위해 안면 비율을  
분류하는  데 딥러닝의  도움이  임상적으로  효과적인지에  대한 연구는  아직 없다. 
- 9 - 
 본 연구는  Willis 안면 계측법의  안모 비율(이하 ‘Willis 비율’)을 정상과  평균 
이하로  분류하는  DCNN  (Neuro -T)의 성능을  평가하는  데 목적이  있다. 이를 위해 
정면 안면 디지털  사진, 측면 안면 디지털  사진, 측방두부 규격방사선사진을  
활용하여 , 표준화된  사진과 주요 안면 부위 크롭 사진의  분류 성능을  비교하 고, 
데이터  증강 기법의 효과와  최적의  학습 샘플 크기를  분석하였다 . 또한, Neuro-
T의 분류 결과 제공이  치과대학  학생과  전공의의  Willis 비율 분류 정확도 에 
미치는  영향을  평가하여 , 임상 및 교육 현장에서의  실질적  활용 가능성을  
제시하고자  한다. 
 
 
 
 
 
 
 
 
 
 
 
 
 
- 10 - 
 II. 재료 및 방법  
 
 본 연구는  전남대학교  치과병원  생명의학연구윤리심의위원회의  승인을  받고 
시행되었 다(CNUDH, IRB No. CNUDH -2025 -006).  
 
1. 데이터  수집 및 라벨링  방법 
 본 연구에  사용된  데이터는  2009년 1월 1일부터  2025년 5월 20일까지  전남대학
교 치과병원에  내원하여  검진 등의 목적으로  정면 안면 디지털  사진(이하 ‘정모
사진’), 측면 안면 디지털  사진(이하 ‘측모사진 ’), 측방두부규격방사선사진을  촬영
한 환자를  대상으로  수집되었다 . 데이터  세트는  정모사진  1,808장, 측모사진  
1,602장, 측방두부규격 방사선사진  1,086장으로  구성되었다 . 
 개인식별정보  유출을  방지하기  위해 정모사진 , 측모사진 , 측방두부규격 방사선사
진을 저장할  때 환자의  성명, 병원 등록번호  등 개인 식별 정보가  기재된  부분을  
제외하였다 . 포함 기준은  만 18세 이상 성인의  정모사진 , 측모사진 , 측방두부규
격방사선사진에서  동공, 비저부 , 구열, 턱끝이  명확하게  보이는  사진으로  설정하
였고, 제외기준은  동공, 비저부 , 구열, 턱끝 중 어느 한 곳이 명확하지  않거나  부
분적으로  잘린 사진, 표준화된  이미지 의 해상도 가 512×512  픽셀보다  작거나 , 주
요 안면 부위를  크롭한  이미지 의 해상도 가 320×320  픽셀보다  작아 해부학적  구
조 식별이  어려운  경우, 그리고  만 18세 미만 아동의  사진으로  설정하였다  
(Figure 1).  
 
 
- 11 - 
  
Figure 1. Inclusion & exclusion criteria  
 
 
2. 정면 안면 디지털  사진, 측면 안면 디지털  사진, 측방두부규격방사선사진에서  
이미지  전처리  방법 
 Willis 비율을  측정할  때, 사진의  촬영 각도 및 위치의  불일치로  인한 측정 오차
가 발생할  수 있으며 (53), 이를 최소화하기  위해 해부학적  기준에  따라 이미지를  
정렬(알씨소프트 , ESTsoft, Seoul, Korea )하였다 . 본 연구에서는  정모사진의  경우 동
공간선 (interpupillary  line)이 수평이  되도록 , 측모사진  및 측방두부 규격방사선사진
의 경우 안와하연 (infraorbitale) 과 외이공  상연 (porion) 을 연결한  Frankfort  수평면
(FH plane) 이 수평이  되도록  회전하였다 . 또한 쇄골 아래 영역과  얼굴 주변의  불
필요한  배경을  잘라내고 , 가로와  세로의  비율을  2:3로 맞추어  이미지를  표준화하
였다. 이를 표준 사진으로  설정하였다 (Figure 2 -a,b,c) .  
이후 Willis 비율 측정에  필요한  동공, 비저부 , 구열, 턱끝을  포함하도록  추가로  
이미지를  잘라 분석에  사용하였다 . 이를 크롭 사진으로  설정하였다 (Figure 2 -d,e,f).  
Table 1 은 본 연구에서  사용된  표준  사진과  크롭  사진의  설정  방법  및 파일  크기

- 12 - 
 를 요약하였다 . 표준 사진의  이미지  크기는  512 x 512 픽셀로 , 크롭 사진의  이미
지 크기는  320 x 320 픽셀로  조정 (resize) 하였다. 이를 통해 각 사진 유형별로  분석
에 사용된  이미지의  크기를  일관되게  표준화하였다 . 
Figure 2. Image standardizing and cropping procedure  
(a) Example of standardized frontal facial image, (b) Example of standardized lateral facial 
image, (c) Example of standardized lateral cephalometric radiograph, (d) Example of  
cropped frontal facial im age, (e) Example of cropped lateral facial image, (f) Example of 
cropped lateral cephalometric radiograph  
 
 
 
 
 
 
 
 
 
 

- 13 - 
 Table 1. Experimental groups classification with preprocessing methods and image sizes.  
Group  Image type  Preprocessing details  Image size  
(pixels)  
Standar -
dized  
images  Standardized 
frontal facial image  Interpupillary line aligned horizontally  
Background cropped  
2:3 aspect ratio  512 × 512 
Standardized lateral  
facial image  FH plane aligned horizontally  
Background cropped  
2:3 aspect ratio  512 × 512 
Standardized lateral  
cephalometric 
radiograph  FH plane aligned horizontally  
Background cropped  
2:3 aspect ratio  512 × 512 
Cropped  
images  Cropped frontal  
facial image  Cropped standardized frontal facial image  
to include pupil, subnasale, rima oris, chin  320 × 320 
Cropped lateral  
facial image  Cropped standardized frontal facial image  
to include pupil, subnasale, rima oris, chin  320 × 320 
Cropped lateral  
cephalometric 
radiograph  Cropped standardized frontal facial image  
to include pupil, subnasale, rima oris, chin  320 x 320  
 
 
3. 자동화된  DCNN  프로그램  
 본 연구에서  사용한  DCNN  프로그램은  자동화된  DCNN 의 일종인  Neuro -T 
version 3.1.0. (Neurocle Inc., Seoul, Korea)  프로그램이다 . Neuro -T 프로그램은  자동으
로 최적화된  DCNN  모델 선택 및 최적의  하이퍼파라미터  튜닝(콘볼루션  레이어  
수, 학습률, 드롭아웃  비율, 배치 크기, 에포크  수 및 최적화  유형)을 검색하여  
자동으로  적용되도록  설계되었다 . 
 본 연구에서는  정모사진에서  Willis 안모 비율을  분류한  Neuro -T 프로그램은  전
- 14 - 
 이 학습을  위해 초기 학습률이  0.002인 적응형  순간 추정 옵티마이저인  Adam 옵
티마이저로  최적화되었고 , 배치크기는  72로, 에포크  수는 256으로 자동으로  설정
되었다 . 측모사진에서  Willis 안모 비율을  분류한  Neuro -T 프로그램은  전이 학습
을 위해 초기 학습률이  0.002인 적응형  순간 추정 옵티마이저인  Adam 옵티마이
저로 최적화되었고 , 배치 크기는  52로, 에포크  수는 262로 자동으로  설정되었다 . 
측방두부규격 방사선사진에서  Willis의 안모 비율을  분류한  Neuro -T 프로그램은  
전이 학습을  위해 초기 학습률이  0.002인 적응형  순간 추정 옵티마이저인  Adam 
옵티마이저로  최적화되었고 , 배치 크기는  72로, 에포크  수는 598로 자동으로  설
정되었다 . 
 
4. Willis 안면 계측법을  이용한  안모 비율 측정 및 그룹 분류 
 정모사진에서  양 동공, 비저부 , 구열, 턱끝 키포인트를  설정하여  Willis 비율을  
측정하였다 . 비저부는  비주 (columella) 와 상순의  피부가  만나는  최하단  지점, 구열
은 상순과  하순이  맞닿는  부분의  중앙 지점, 턱끝은  턱 연조직의  최하단  지점으
로 설정하였다 . 측모사진에서 도 마찬가지로  동공, 비저부 , 구열, 턱끝 키포인트를  
설정하여  Willis 비율을  측정하였으며 , 측방두부 규격방사선사진에서도  동일하게  
연조직에  키포인트를  설정하여  Willis 비율을  측정하였다 (Figure 3 ). 측방두부규격
방사선사진에서  실제  동공은  명확하게  식별하기  어려우므로 , 안구에서  가장  전방
으로  돌출된  지점을  동공의  기준점으로  설정하였다 .  
Willis 비율은  서양인을  기준으로  제시된  지표이므로 , 본 연구에서는  한국인을  
대상으로  한 이전의  인체 계측 논문을  참고하여 (54),(55),(56),(57) 기준값을  0.92로 
설정하였다 . 따라서  Willis 비율이  0.92 이상인  경우 정상 그룹으로  분류하고 , 0.92 
- 15 - 
 미만인  경우 평균 이하 그룹으로  분류하였다 . Table 2 는 정모사진 , 측모사진 , 측방
두부규격방사선사진에서  정상  그룹과  평균  이하  그룹으로  라벨링  한 사진  수를  
나타낸다 ..  
Figure 3 은 정모사진 , 측모사진 , 측방두부규격방사선사진에서  동공 , 비저부 , 구열 , 
턱끝의  해부학적  키포인트를  설정하고 , 이들  사이의  거리를  측정하여  Willis 비율
을 산출하는  과정을  도식화한  것이다 . 산출된  Willis 비율에  따라 정상 그룹 (x ≥ 
0.92)과 평균 이하 그룹 (x < 0.92) 으로 분류하였으며 , 분류된  결과는  전남대학교  치
과병원  보철과  전문의가  검증하여  Willis 비율 분류의  gold standard 로 설정하였다 .  
 
Table 2. Number of labeled images for frontal facial images, lateral facial images, and lateral 
cephalometric radiographs.  
Group  Frontal facial images  Lateral facial images  Lateral cephalometric 
radiographs  
Normal group  
(x≥0.92) 1,107  1,096  916 
Lower group  
(x<0.92)  701 506 170 
Total  1,808  1,602  1,086  
x : Willis ratio  
 
 
 
 
 
 
- 16 - 
  
Figure 3. How to label  4 keypoints (pupil, base of nose, rima oris, chin),  classify Willis ration, 
and establish  the gold standard . The author (S.H.K.) provided the facial photograph and 
consented to its publication.  
 
 
 
 

- 17 - 
 5. Neuro -T 프로그램 의 Willis 안면 계측법 분류 정확도  평가 
 훈련 데이터에  데이터  불균형이  존재할  경우, 모델은  다수 클래스에  과적합되기  
쉬워 전반적인  성능이  저하될  수 있다 (58). 따라서  본 연구에서는  다수 클래스인  
정상 그룹을  언더샘플링 (undersampling )하여 두 클래스의  데이터  불균형을  맞추었
다(59).  본 연구에서는  데이터  불균형이  모델 성능에  미치는  영향을  확인하기  위
해 언더샘플링을  통해 클래스  간 데이터  수를 맞춘 성능을  비교하 였다. 정모사진
의 경우 정상 그룹 1,107장 중 700장을 무작위  추출하여  언더샘플링  하였으며 , 
이로 인해 407장의 데이터가  제외되었다 . 측모사진의  경우 정상그룹  1,096장 중 
500장을 무작위  추출하여  언더샘플링  하였으며 , 이로 인해 596장의 데이터가  제
외되었다 . 측방두부 규격방사선사진의  경우정상  그룹  916장 중 170장을  무작위  추
출하였으며 , 이로  인해  746장의  데이터가  제외되었다 . 이 방식으로  추출된  정모
사진 1,400장, 측모사진  1,000장, 측방두부 규격방사선사진  340장을 각각 표준 사
진과 크롭 사진으로  나누어  2가지 그룹으로  라벨링 한 후 무작위로  섞어 85%는 
훈련용 , 15%는 테스트  데이터로  나누었다 (Table 3).  이 사진들을  Neuro -T 프로그램
을 훈련시켜  테스트한  사진들에서  혼동 행렬(Confusion Matrix )을 구하였다 (Table 4)  
 
 
 
 
 
 
 
 
- 18 - 
 Table 3. Distribution of frontal facial images, lateral facial images, and lateral cephalometric 
radiographs for training, testing in Neuro -T program  
Group Frontal facial images  Training (85%)  Test (15%)  
Normal group  
(x≥0.92)  700 595 105 
Lower group  
(x<0.92)  700 595 105 
Total 1,400 1,190  210 
Group Lateral facial images  Training (85%)  Test (15%)  
Normal group  
(x≥0.92)  500 425 75 
Lower group  
(x<0.92)  500 425 75 
Total 1,000 850 150 
Group Lateral cephalometric 
radiographs  Training (85%)  Test (15%)  
Normal group  
(x≥0.92)  170 144 26 
Lower group  
(x<0.92)  170 144 26 
Total 340 288 52 
 
 
 
 
 
 
 
- 19 - 
 Table 4. Confusion matrix for classification of Willis ratio  
Confusion matrix  True label  
Normal group  Lower group  
Prediction  Normal group  a (TP)  b (FP)  
Lower group  c (FN)  d (TN)  
 
혼동 행렬을  통해 Neuro -T 프로그램의  Willis 비율을  정상 그룹과  평균 이하 
그룹으로  분류하는  정확도 (Accuracy) 를 계산하였다 . 또한, 각 그룹의  
정밀도 (Precision) , 재현율 (Recall) , F1 점수 (F1 score) 를 계산하고  평균을  내어 전체 
정밀도 , 재현율 , F1 점수를  계산하였다 (Table 5).  
 
Table 5. Accuracy, Precision, Recall, F1 score  
Group  Precision (%)  Recall (%)  F1 score (%)  Accuracy (%)  
Normal 
group  a
a + c a
a + b 2a
2a + b + c a + d
a + b + c +  d 
Lower 
group  d
b + d d
c + d 2d
2d + b + c 
  
정확도는  전체 데이터  중 맞게 예측한  것의 비율이다 . 이는 전체 예측 중에서  
올바르게  예측한  비율로  계산된다 . 정밀도는  모델이  특정  그룹으로  예측한  것 중 
실제로  그 그룹에  속하는  것의 비율을  뜻하며 , 재현율은  실제로  특정 그룹에  속
하는 것에서  모델이  그 그룹으로  예측한  것의 비율을  뜻한다 . F1 점수는  정밀도
와 재현율의  조화평균  점수로  불균형  클래스  분포 상황에서  특히 유용하며 , 두 
지표의  균형을  반영하여  모델의  예측 성능을  종합적으로  평가한다 (60). Willis 비
율을 분류한  2개의 각 그룹에서의  정밀도 , 재현율 , F1 점수를  계산하고  평균을  
- 20 - 
 내어 전체 정밀도 , 재현율 , F1 점수를  계산하였다 . 
 
가. 표준 사진과  크롭 사진의  성능 비교 및 Grad -CAM 분석 
정모사진  1,400장, 측모사진  1,000장, 측방두부 규격방사선사진  340장 각각에서  
표준사진과 크롭 사진 간의 분류 성능을  정밀도와  재현율의  조화평균인  F1 점수
로 비교하였다 . 표준 사진에서  크롭 사진보다 성능이  낮은 이유를  확인하기  위해 
Neuro -T 프로그램의  Grad -CAM  (Gradient -weighted Class Activation Mapping) 을 분석
하였다 . 
 
나. 사진 유형별  성능 비교 및 Grad -CAM 기반 오류 원인 분석 
사진의  종류(정모사진  1,400장, 측모사진  1,000장, 측방두부 규격방사선사진  340
장)에 따른 DCNN 의 분류 성능을  정밀도와  재현율의  조화평균인  F1 점수로  비교
하였다 . 또한 Neuro -T 프로그램의  오분류가  주요 안면 부위가  아닌 부위에  초점
을 두었기  때문인지 , 혹은 Willis 비율 분류 경계에  위치한  사진을  다른 그룹으로  
잘못 판단했기 때문인지  등 어떤 요인으로  인한 것인지  파악하기  위해 크롭한  
정모사진 , 측모사진 , 측방규격두부방사선사진에서  Grad -CAM 분석을  시행하였다 . 
 
다. 데이터 증강 기법이  분류 정확도 에 미치는  영향 평가 
 크롭 정모사진에서  Willis 비율 분류에  필요한  해부학적  키포인트 (동공, 비저
부, 구열, 턱끝)의 기하학적  관계를  보존한 데이터  증강을  위해, 크롭 정모사진  
500장에 대해 색상 기반 데이터  증강 기법을 적용하였다 (44). 총 4가지 색상 기
반 데이터  증강 조합(대비 +200% , 감마 -60%, 색조 -20%, 채도 +100% )을 단독 
- 21 - 
 및 조합하여  자동화된  DCNN 을 훈련시켰다 (Figure 4-a,b,c,d,e ). 그리고  그 분류 결
과를 F1 점수로  비교분석하였다 .  
또한 크롭 정모사진  500장에 대해 좌우반전  기법, 회전 기법(시계방향으로  2° 
회전)을 각각 적용하여  데이터  증강 후 자동화된  DCNN 을 훈련시켰다 (Figure 4 -
a,d,e). 그리고  그 분류 결과를  F1 점수로 비교분석하였다 .  
 
Figure  4. Examples of data augmentation. (a) Original images, (b) Contrast adjustment, (c) 
Gamma adjustment, (d) Hue adjust ment, (e) Saturation adjustment, (f) Horizontal flipping, (g) 
2° clockwise rotation.  
 
라. 학습 데이터  양이 분류 정확도에  미치는  영향 평가 
 Neuro -T 프로그램에서  Willis 비율을  정확히  분류할  수 있는 최적의  사진 개수
를 찾기 위해 크롭 정모사진의  개수를  100개씩 증가시키며  그룹당  700개까지  학
습을 진행하였다 . 학습 곡선 (Learning curve) 은 DCNN  프로그램의  학습 진행 상황

- 22 - 
 과 정확도  향상을  시각적으로  보여주는  중요한  도구로 , 훈련 데이터  세트의  크기
와 분류 정확도  간의 관계를  나타낸다 (61). 이 곡선에서  DCNN  프로그램의  정확
도 증가량을  분석하여  DCNN  프로그램이  최종 정확도에  도달하는  사진 개수를  
계산하였다 . 
 
6. 딥러닝의  지원이  학생과  전공의의  분류 정확도에  미치는  영향 평가 
 본 연구에서는  Neuro -T 프로그램의  도움이  치과대학  학생과  전공의의  Willis 비
율 분류 정확도에  미치는  영향을  평가하기  위해 설문지를  제작하였다 (Figure  5). 
설문조사는  구글폼을  활용하여  두 단계로  진행되었으며  첫 번째 단계는  표준화
된 정모사진  20장, 표준화된  측모사진  20장, 표준화된  측방두부 규격방사선사진  
20장씩 총 60장의 익명화된  사진을  제공하고 , Willis 비율이  정상인지  평균 이하
인지를  선택하는  2지선다형  객관식  문제로  구성하였다 . 그리고  두 번째 단계는  
첫 번째 단계와  동일한  환자의  표준화된  정모사진  20장, 표준화된  측모사진  20장, 
표준화된  측방두부규격방사선사진  20장씩 총 60장의 익명화된  사진을  제공하였
으며, 여기에  Neuro -T의 분류 예측값과  정확도  정보를  함께 제시하였다. 문항 출
제 시 딥러닝  프로그램의  전체 정확도에  비례하여  Neuro -T가 실제로  정확히  분
류한 사례와  오분류한  사례를  포함시켜  딥러닝  프로그램의  도움 전후로  분류 정
확도의  변화를  비교하였다 . 또한 다양한  Willis 비율 구간을  고려하여  문제를  구
성함으로써 , Willis 비율에  따른 분류 정확도와  딥러닝  프로그램의  지원 효과를  
평가하였다 . 연구의  충분한  통계적  검정력을  확보하기  위해 G*Power software  
(version 3.1, Heinrich -Heine -Universitat Dusseldorf, Dusselodrf, Germany) 를 사용하여  표
본 크기를  산출하였다 . 설정값은  효과 크기 0.5, 유의수준  0.05, 검정력 (power)  0.8
- 23 - 
 로 설정하였으며 , 이에 따라 최소 27명의 표본이  필요하다는  결과를  얻었다 . 따
라서 해당 설문조사는  치과대학  학생 31명, 전공의  30명을 대상으로 시행되었다 . 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
- 24 - 
 
 
Figure 5. Google Form questionnaire used for  Willis ratio  classification (The facial images 
were provided to the students without any masking of the eye region). (a) Introduction text on 
the Google Form survey , (b) Description of the Willis method , (c) Explanation of the 
questionnaire procedure, (d) Questionnaire without deep learning result, (e) Questionnaire 
with deep learning result  

- 25 - 
 설문조사  데이터는  Microsoft Excel (version 360, Microsoft, Redmond, WA, USA) 을 
사용하여  스프레드시트로  수집 및 관리하였다 . 모든 통계 검정은  Python 의 scipy 
라이브러리의  stats 모듈을  사용하여  시행하고 , p값이 0.05 미만인  경우 통계적  유
의성을  갖는 것으로  간주하였다 (62). 치과대학  학생과  전공의의  점수가  정규  분
포를  따르는지  확인하기  위해  Shapiro -Wilk 검정을  시행한  결과 , 전공의의  딥러닝  
프로그램의  지원  후 측모사진  분류  점수 , 전공의의  딥러닝  프로그램  지원  전 측
방두부규격방사선사진  분류  점수 , 학생의  딥러닝  프로그램  지원  전 정모사진  점
수, 학생의  측방두부규격방사선사진  분류  점수가  정규분포를  따랐고 , 나머지  점
수의  경우  정규분포를  따르지  않음을  확인하였다 (63). 이러한  결과를  바탕으로  
치과대학  학생과  전공의  간 Willis 비율 분류의  정확도에  유의한  차이가  있는지  
평가하기  위해 정규 분포를  따르는  조건에서  독립 표본 t-검정 (independent t-test)
을, 정규 분포를  따르지  않는 조건에서  Mann -Whitney U 검정을  시행하였다 . 또한, 
딥러닝  프로그램의  도움을  받기 전후의  치과대학  학생과  전공의 의 정확도가  유
의하게  증가하였는지  평가하기  위해 정규 분포를  따르는  조건에서  대응 표본 t-
검정 (paired t-test)을, 정규 분포를  따르지  않는 조건에서  Wilcoxon Signed -Rank 검
정을 시행하였다 (64). 
 전공의  30명 중 보철과  전공의는  11명, 보철과가  아닌 전공의는  19명이었으며 , 
보철과  전공의와  보철과가  아닌 전공의의  점수가  정규 분포를  따르는지  확인하
기 위해 Shapiro -Wilk 검정을  시행하였다 . 그 결과, 보철과  전공의의  딥러닝  프로
그램 지원 후 분류 점수, 보철과가  아닌 전공의의  딥러닝  프로그램  지원 전 측모
사진 분류 점수, 보철과가  아닌 전공의의  딥러닝  프로그램  지원 후 측방두부규격
방사선사진의  분류 점수가  정규분포를  따르지  않았고 , 나머지  점수의  경우 정규
- 26 - 
 분포를  따르는  것을 확인하였다 (63). 이러한  결과를  바탕으로  보철과  전공의와  
보철과가  아닌 전공의  간 Willis 비율 분류의  정확도에  유의한  차이가  있는지  평
가하기  위해 정규 분포를  따르는  조건에서  독립 표본 t-검정을 , 정규 분포를  따
르지 않는 조건에서  Mann -Whitney U 검정을  시행하였다 . 또한, 딥러닝  프로그램
의 도움을  받기 전후의  보철과  전공의와  보철과가  아닌 전공의의  정확도가  유의
하게 증가하였는지  평가하기  위해 정규 분포를  따르는  조건에서  대응 표본 t-검
정을, 정규 분포를  따르지  않는 조건에서  Wilcoxon Signed -Rank 검정을  시행하였
다(64). 
 
 
 
 
 
 
 
 
 
 
 
- 27 - 
 III. 결과  
 
1. Neuro -T 프로그램의  Willis 안면 계측법의  분류 정확도  평가 
본 연구에서는  실험 시작 전 크롭 정면사진에서  불균형한  데이터  세트를  이용
하여 Neuro -T 프로그램을  훈련시켰고 , 그 결과 무작위  언더샘플링으 로 균형을 맞
춘 경우보다  재현율과  F1 점수가  상대적으로  낮았다 (Table 6 ). 따라서  본 연구에
서는 무작위  언더샘플링을  통해 그룹 수의 균형을  맞추어  훈련을  진행하였다 . 
 
Table 6. Comparison of accuracy between imbalance and balanced datasets in cropped frontal 
facial images.  
Condition  Group distribution  
(normal : lower)  Accuracy 
(%) Precision 
(%) Recall 
(%) F1 score 
(%) 
Imbalanced  1060 : 503  89.30  88.60 86.50 87.50 
Balanced  500 : 500  90.00 90.20 90.00 90.10 
 
정모사진  1,400장, 표준 측모사진  1,000장, 측방두부 규격방사선사진  340장을 각
각 표준사진과  크롭사진으로  구성하여  Willis 비율에  따라 2가지 그룹으로  분류
한 후, Neuro -T 프로그램으로  훈련시켜  혼동행렬을  구하였고 (Table 7), 이를 통해 
정확도 , 정밀도 , 재현율 , F1 점수를  계산하였다 (Table 8 ).  
Neuro -T의 혼동행렬을  분석한  결과, 표준 정모사진에서  잘못 분류한  사진은  210
장 중 32장(15.24%)이었고 , 표준 측모사진에서  잘못 분류한  사진은  150장 중 31
장(20.67%) 이었고 , 표준 측방두부규격 방사선사진에서  잘못 분류한  사진은  52장 
중 19장(36.54%)이었다 . 또한 크롭 정모사진에서  잘못 분류한  사진은  210장 중 
- 28 - 
 21장(10%)이었고 , 크롭 측모사진에서  잘못 분류한  사진은  150장 중 24장(16%) 이
었고, 크롭 측방두부규격 방사선사진에서  잘못 분류한  사진은  52장 중 14장 
(26.92%) 이었다  (Table 7). 
 
Table 7. Confusion matrix of Neuro -T program in standardized & cropped frontal facial 
images, lateral facial images, and lateral cephalometric radiographs  
Standard - 
ized 
frontal 
facial 
images  Actual  
/Predicted  Normal  Lower  Cropped  
frontal 
facial 
images  Actual  
/Predicted  Normal  Lower  
Normal  102 3 Normal  96 9 
lower  29 76 Lower  12 93 
Standard - 
ized 
lateral 
facial  
images  Actual  
/Predicted  Normal  Lower  Cropped  
 lateral 
facial  
images  Actual  
/Predicted  Normal  Lower  
Normal  60 15 Normal  60 15 
lower  16 59 lower  9 66 
Standard - 
ized 
lateral 
cephalo - 
metric 
radiogra - 
phs Actual  
/Predicted  Normal  Lower  Cropped  
 lateral 
cephalo - 
metric 
radiogra - 
phs Actual  
/Predicted  Normal  Lower  
Normal  13 13 Normal  16 10 
lower  6 20 lower  4 22 
 
표준  정모사진에서  Willis 비율을  2가지  그룹으로  분류한  Neuro -T 프로그램은  
정확도  84.76%, 정밀도  87.00%, 재현율  84.80%, F1 점수  85.90%를 달성하였다 . 표
준 측모사진의  경우  정확도  79.33%, 정밀도  79.30 %, 재현율  79.30%, F1 점수  
79.30% 를 달성하였고 , 표준  측방두부규격 방사선사진의  경우  정확도  67.30%, 정밀
- 29 - 
 도 71.10%, 재현율  67.30%, F1 점수  69.10% 를 달성하였다 . 크롭  정모사진에서  
Willis 비율을  2가지  그룹으로  분류한  Neuro-T 프로그램은  정확도  90.00%, 정밀도  
90.20%, 재현율  90.00%, F1 점수  90.10% 를 달성하였다 . 크롭  측모사진의  경우  정확
도 84.00%, 정밀도  84.20%, 재현율  84.00%, F1 점수  84.10% 를 달성하였고 , 크롭  측
방두부규격 방사선사진의  경우  정확도  73.07%, 정밀도  74.40%, 재현율  73.10%, F1
점수  73.70% 를 달성하였다 (Table 8). 
 
Table 8. Accuracy, precision, recall, and F1 score of Confusion matrix of standardized & 
cropped frontal facial images, lateral facial images, and lateral cephalometric radiographs  
Images  Standardized  Cropped  
Frontal 
facial 
images  Lateral 
facial 
images  Lateral 
cephalometric 
radiographs  Frontal 
facial 
images  Lateral 
facial 
images  Lateral 
cephalometric 
radiographs  
Accuracy (%)  84.76 79.33 63.46  90.00 84.00 73.07 
Precision (%)  87.00 79.30 64.50 90.00 84.20 74.40 
Recall (%)  84.80 79.30 63.50  90.00 84.00 73.10 
F1 score (%)  85.90 79.30 64.00  90.00 84.10 73.70 
 
 
가. 표준 사진과  크롭 사진의  성능 비교 및 Grad -CAM 분석 결과 
정모사진 , 측모사진 , 측방두부규격 방사선사진에서  Neuro-T 프로그램을  이용하
여 표준 사진을  분류한  모델과  크롭 사진을  분류한  모델의 성능을  비교한 결과, 
모든 유형의  사진에서  표준 사진보다  크롭 사진을  분류한  모델의  정확도 , 정밀도 , 
재현율 , F1 점수가  더 높았다 (Table 8). 표준 사진에서의  성능이  크롭 사진에서의  
성능보다 낮은 이유를  찾기 위해 Neuro -T 프로그램이  정확하게  판단한  사진과  
- 30 - 
 잘못 판단한  사진의  Grad -CAM을 분석하였다 . 그 결과, 표준 사진에 서는 잘못 판
단한 사진 뿐 아니라  정확하게  판단한  사진에서도  빨간색  관심영역이  넓게 퍼진 
사례가  다수 관찰되었으며 , 특히 측방두부규격방사선사진 의 경우 두경부  골격 구
조로도  빨간색  관심영역이  분산되는  경향이  관찰되었다 . 크롭 사진의  경우 정확
하게 판단한  사진에서는  주요 안면 부위에  빨간색  관심영역이  설정되었다 (Figure 
6).  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
- 31 - 
 Figure 6 . Comparison of Grad -CAM between standardized and cropped images. (a) Accurate 
detection in standardized frontal facial images, (b) Inaccurate detection in standardized frontal 
facial images, (c) Accurate detection in cropped frontal facial images , (d) Inaccurate detection 
in cropped frontal facial images , (e) Accurate detection in standardized lateral facial images , 
(f) Inaccurate detection in standardized lateral facial images , (g)  Accurate detection in 
standardized lateral facial images , (h) Inaccurate d etection in standardized lateral facial images , 
(i) Accurate detection in standardized lateral cephalometric radiographs , (j) Inaccurate 
detection in standardized lateral cephalometric radiographs , (k) Accurate detection in cropped 
lateral cephalometric ra diographs, (l) Inaccurate detection in cropped  lateral cephalometric 
radiographs  

- 32 - 
 나. 사진 유형별  성능 비교 및 Grad -CAM 기반 오류 원인 분석 결과 
 정모사진 , 측모사진 , 측방두부규격 방사선사진에서  Neuro -T 프로그램을  이용하여  
표준 사진을  분류한  모델과  크롭 사진을  분류한  모델의  성능을  비교한  결과, 표
준 사진을  분류한  모델과  크롭 사진을  분류한  모델 모두에서  정모사진의  경우 
정확도 , 정밀도 , 재현율 , F1 점수가  모두 가장 높았고 , 그 다음으로  측모사진 , 측
방두부규격 방사선사진  순으로  정확도 , 정밀도 , 재현율 , F1 점수가  낮아지는  경향
을 보였다 (Table 8).  
크롭 사진에서  Neuro -T 프로그 램의 분류 오류를  Grad -CAM을 분석하여 오류 패
턴을 도출하였다 . 크롭 정모사진의  경우 1,400장 중 210장이 테스트용  데이터로  
사용되었고 , 이 중에서  Neuro -T가 잘못 판단한  21장을 분석하였다 . Grad -CAM 분
석을 통해 오류 패턴을  세분화한  결과 21장 중 분류 기준값의  경계에  위치한  사
진은 2장(9.52%)이었으며  다른 부위에  초점을  두어 잘못 분류한  사진이  19장
(90.48 %)이었다 (Figure 7). 
 크롭  측모사진  1,000장 중 150장이 테스트용  데이터로  사용되었고 , 이 중에서  
Neuro -T가 잘못 판단한  24장을 분석하였다 . Grad -CAM 분석을  통해 오류 패턴을  
세분화한  결과 24장 중에서  분류 기준값의  경계에  위치한  사진은  2장(8.33%) 이었
으며 다른 부위에  초점을  두어 잘못 분류한  사진이  17장(70.83%), 관심영역이  넓
게 퍼진  사진이  5장(20.84%) 이었다 (Figure 8 ). 
 
- 33 - 
 Figure 7. Distribution of misclassified cropped frontal facial images analyzed using Grad -
CAM  
Boundaries: For misclassifications due to Willis ratio at classification boundaries (0.91, 0.92)  
Misinterpreting features: For misclassification due to focusing on irrelevant area  
Figure 8. Distribution of misclassified cropped lateral facial images analyzed using Grad -
CAM  
Boundaries: For misclassifications due to Willis ratio at classification boundaries (0.91, 0.92)  
Broad ROI: For misclassification due to overly broad region of interest  
Misinterpreting features: For misclassification due to focusing on irrelevant area  

- 34 - 
 크롭 측방두부규격 방사선사진  340장 중 52장이 테스트용  데이터로  사용되었고 , 
이 중에서  Neuro -T가 잘못 판단한  14장을 분석하였다 . Grad -CAM 분석을  통해 오
류 패턴을  세분화한  결과 14장 중 분류 기준값의  경계에  위치한  사진은  2장
(14.29%) 이었으며  다른 부위에  초점을 두어 잘못 분류한 사진이 11장(78.57%),  관
심영역이 넓게 퍼진 사진이 1장(7.14% )이었다 (Figure 9). 
 
Figure 9. Distribution of misclassified cropped lateral cephalometric  radiographs analyzed 
using Grad -CAM 
Boundaries: For misclassifications due to Willis ratio at classification boundaries (0.91, 0.92)  
Broad ROI: For misclassification due to overly broad region of interest  
Misinterpreting features: For misclassification du e to focusing on irrelevant area  
 
다, 데이터  증강 기법이  분류 정확도에  미치는  영향 
 크롭 정모사진에  대해 대비, 감마, 색조, 채도를  조정하여  데이터  증강을  시행
하였고 , 총 4가지 증강 조합(대비 +200% , 감마 -60%, 색조 -20%, 채도 +100% , 
Figure 3)을 단독 및 조합하여  Neuro -T 프로그램을  훈련시킨  결과 원본 이미지로

- 35 - 
 만 훈련시킨  모델의  F1 점수가  가장 높았다 (Table 9 ). 
 
Table 9.  Accuracy, precision, recall, and F1 score trained with a combination of augmentation 
techniques. A red mark signifies that the corresponding augmentation technique was used.  
N (per 
group)  O C G H S Accuracy 
(%) Precision  
(%) Recall  
(%) F1 score  
(%) 
500      90.00 90.20  90.00 90.10 
1,000      83.66 84.50 83.80 84.10 
     93.00 83.00  83.00 83.00 
     76.66 78.30 76.70 77.50 
     75.33 77.50  75.30  76.40 
1,500      79.77 81.30  79.80 80.50 
     84.00 84.30 84.00 84.10 
     83.77 84.00 83.80 83.90 
     74.00 76.50 74.00 75.20 
     81.77 82.40 81.80 82.10 
     78.66 79.30 78.70 79.00 
2,000      86.33 86.60 86.30 86.40 
     84.00 84.10 84.00 84.00 
     84.33 84.40 84.30 84.30 
     82.16 82.40 82.20 82.30 
2,500      79.20 80.10 79.20 79.60  
O: original image, C: data augmentation with contrast adjustment, G: data augmentation with 
gamma adjustment, H: data augmentation with hue adjustment, S: data augmentation with 
saturation adjustment  
 
 
 
 
- 36 - 
  또한 크롭 정모사진에  대해 좌우반전  기법의  데이터  증강 및 2°의 시계방향  회
전 기법의  데이터  증강을  각각 시행하였고 , Neuro-T 프로그램 을 훈련시킨  결과 
두 경우 모두 원본 이미지로만  훈련시킨  모델의  F1 점수가  가장 높았다 (Table 10).  
 
Table 10. Accuracy, precision, recall, and F1 score trained with flipping, rotation 
augmentation techniques (horizontal flipping, 2° clockwise rotation ). 
N  
(per group)  Augmentation  
technique  Accuracy 
(%) Precision 
(%) Recall  
(%) F1 score 
(%) 
500 - 90.00 90.20  90.00 90.10 
1,000 Horizontal 
flipping  86.66 87.20  86.70 86.90 
2° clockwise 
rotation  82.33 82.30 82.30 82.30 
 
 
라. 학습 데이터  양이 분류 정확도에  미치는  영향 
 크롭 정모사진에서  Willis 비율을  정확히  분류할  수 있는 최적의  사진 개수를  
찾기 위해 그룹 당 사진 개수를  100개씩 증가시키며  Neuro-T 프로그램 의 정확도
를 평가하였다 . 그룹 당 사진 수가 증가함에  따라 Neuro-T 프로그램 의 분류 정확
도가 개선되었다 . Neuro -T 프로그램에서  그룹 당 700개씩 분류한  정확도는  90.00%
에 도달하였다 (Table 11). 학습곡선  분석 결과, 500장 이후 정확도는  포화 상태를  
보였으며 (Figure 10 ), 이는 최적 샘플 크기로  판단된다 .  
 
 
 
- 37 - 
 Table 11. The accuracy based on the number of cropped frontal facial images in Neuro -T 
program.  
Number of cropped frontal facial 
images  Accuracy 
(%) Precision 
(%) Recall 
(%) F1 score 
(%) 
Total  per group  
200 100 70.00  70.00  70.00  70.00  
400 200 81.66  82.60  81.70  82.10  
600 300 84.44  84.50  84.40  84.40  
800 400 89.16  89.40  89.20  89.30  
1,000  500 90.00  90.20  90.00  90.10  
1,200  600 90.00  90.10  90.00  90.00  
1,400  700 90.00  90.00  90.00  90.00  
 
 
 
Fig 10 . Training curve of Neuro -T in cropped frontal facial images  
 

- 38 - 
 2. 딥러닝  지원이  학생과  전공의의  분류 정확도에  미치는  영향 
가. 딥러닝의  지원 전 분류 정확도  
 딥러닝  프로그램 의 도움을  받기 전에 치과대학  학생과  전공의 의 평균 정확도는  
정모사진에서는  각각 55.81 % (±9.23)와 68.00% (±10.47 ), 측모사진에서는  각각 
54.84 % (±9.17)와 62.33% (±11. 12), 측방두부 규격방사선사진에서는  각각 55.48 %  
(±12.41 )와 67.67% (±11.35 )였다 (Table 1 2).  
 전공의  30명 중 보철과  전공의가  11명, 보철과가  아닌 전공의가  19명 포함되었
다. 딥러닝  프로그램의  도움을  받기 전에 보철과  전공의와  보철과가  아닌 전공의
의 평균 정확도는  정모사진에서는  각각 70.91 % (±9.70)와 66.32 % (±10.78 ), 측모사
진에서는  각각 64.55 % (±10.11 )와 61.05% (±11.74 ), 측방두부규격방사선사진에서는  
각각 69.09 % (±13.00 )와 66.84 % (±10.57 )였다 (Table 1 3). Willis 비율을  분류하는  
Neuro -T 프로그램의  정확도는  정모사진에서  90.00%,  측모사진에서  84.00%,  측방
두부규격방사선사진에서  73.07% 였으며, 이는 치과대학  학생과  전공의의  평균 정
확도를  능가하였다 . 
 
나. 딥러닝의  지원 후 분류 정확도  
 딥러닝  프로그램의  지원 후, 치과대학  학생의  평균 정확도는  정모사진에서  
76.13 % (±12.50 ), 측모사진에서  72.42%  (±13.59 ), 측방두부규격방사선사진에서  67.42 % 
(±9.30)로 딥러닝의  지원 전보다  정확도가  향상되었다 . 전공의의  딥러닝  프로그램  
지원 후 평균 정확도는  정모사진에서  76.50% (±6.71), 측모사진에서  74.50% 
(±10.93 ), 측방두부규격방사선사진에서  70.17% (±6.23)로 딥러닝의  지원 전보다  정
확도가  향상되었다 (Table 12 ). 
- 39 - 
 치과대학  학생과  전공의의  점수가  정규  분포를  따르는지  확인하기  위해  
Shapiro -Wilk 검정을  시행한  결과 , 전공의의  딥러닝  프로그램의  지원  후 측모사진  
분류  점수 , 전공의의  딥러닝  프로그램  지원  전 측방두부규격방사선사진  분류  점
수, 학생의  딥러닝  프로그램  지원  전 정모사진  점수 , 학생의  측방두부규격방사선
사진  분류  점수가  정규분포를  따랐고 , 나머지  점수의  경우  정규분포를  따르지  않
음을  확인하였다 (63). 이러한  결과를  바탕으로  정규 분포를  따르는  조건에서  독립 
표본 t-검정을 , 정규 분포를  따르지  않는 조건에서  Mann -Whitney U 검정을  사용
하여 치과대학  학생과  전공의  그룹의  정확도  차이를  평가하였다 . 그 결과, 딥러
닝의 지원 전의 경우 모든 유형의  사진에서  전공의 가 학생보다  통계적으로  유의
하게 분류 점수가  높았다 (p<0.05) . 딥러닝의  지원 후에는  모든 유형의  사진에서 
학생과  전공의  간에 유의한  차이는  없었다 (정모사 진 p=0.604 , 측모사진  p=0.714, 
측방두부 규격방사선사진  p=0.520). 
또한, 딥러닝  프로그램의  도움을  받기 전후의  학생과  전공의의  정확도가  유의
하게 증가하였는지  평가하기  위해 정규 분포를  따르는  조건에서  대응 표본 t-검
정을, 정규 분포를  따르지  않는 조건에서  Wilcoxon Signed -Rank 검정을  시행하였
다. 그 결과, 치과대학  학생은 모든 유형의  사진에서  딥러닝의  지원 후 분류 정
확도가  유의하게  향상되었다 (p<0.05) . 전공의의  경우 정모사진 , 측모사진에서  딥
러닝의  지원 후 분류 정확도가  유의하게  향상되었으며 (p<0.05) , 측방두부규격방사
선사진에서는  딥러닝의  도움을  받기 전후 정확도에  통계적으로  유의한  차이가  
없었다 (p=0.471 ). 또한 딥러닝의  도움을  받은 후 정확도  증가량은  정모사진과  측
방두부규격방사선사진에서  학생이  전공의보다  유의하게  높았으며 (p<0.05) , 측모사
진에서는  두 집단 간 유의한  차이가  관찰되지  않았다 (p=0.094) (Figure 1 1). 
- 40 - 
 딥러닝  프로그램의  지원 후, 보철과  전공의의  평균 정확도는  정모사진에서  
76.36% (±3.93), 측모사진에서  78.18% (±7.83), 측방두부규격방사선사진에서  72.27% 
(±7.20)로 딥러닝의  지원 전보다  정확도가  향상되었다 . 보철과가  아닌 전공의의  
딥러닝  프로그램  지원 후 평균 정확도는  정모사진에서  76.58% (±8.00), 측모사진
에서  72.37% (±12.06 ), 측방두부규격방사선사진에서  68.95% (±5.42)로 딥러닝의  지
원 전보다  정확도가  향상되었다 (Table 1 3). 
보철과  전공의와  보철과가  아닌 전공의의  점수가  정규 분포를  따르는지  확인하
기 위해 Shapiro -Wilk 검정을  시행하였다 . 그 결과, 보철과  전공의의  딥러닝  프로
그램 지원 후 분류 점수, 보철과가  아닌 전공의의  딥러닝  프로그램  지원 전 측모
사진 분류 점수, 보철과가  아닌 전공의의  딥러닝  프로그램  지원 후 측방두부규격
방사선사진의  분류 점수가  정규분포를  따르지  않았고 , 나머지  점수의  경우 정규
분포를  따르는  것을 확인하였다 (63). 이러한  결과를  바탕으로  정규 분포를  따르
는 조건에서  독립 표본 t-검정을 , 정규 분포를  따르지  않는 조건에서  Mann -
Whitney U 검정을  사용하 여 치과대학  학생과  전공의  그룹의  정확도  차이를  평가
하였다 . 그 결과 , 딥러닝  지원  전과  후의  모든  유형의  사진에서  보철과  전공의와  
보철과가  아닌  전공의  간에 통계적으로  유의한  차이는  없었다 (p<0.05) . 
딥러닝  프로그램의  도움을  받기 전과 후에 보철과  전공의와  보철과가  아닌 전
공의의  정확도  변화를  평가하기  위해 정규 분포를  따르는  그룹은  모두  검정인  
대응  표본  t-검정 , 정규  분포를  따르지  않는  비모수  검정인  Wilcoxon Signed -Rank 
검정을  사용하였다 . 그 결과, 정모사진에서  보철과  전공의의  정확도  변화, 측방
두부규격방사선사진에서  보철과  전공의와  보철과가  아닌 전공의의  정확도  변화
는 유의한  차이가  없었고 (각각  p=0.125 , p=0. 426, p=0.731), 정모사진에서  보철과가  
- 41 - 
 아닌 전공의의  정확도  변화, 측모사진에서  보철과  전공의와  보철과가  아닌 전공
의의 정확도  변화는  유의한  차이가  있었다 (p<0.05) . 또한 딥러닝의  도움을  받은 
후 보철과  전공의의  정확도  증가량과  보철과가  아닌 전공의의  정확도  증가량  간
에는 통계적으로  유의미한  차이가  없었다 (정모사 진 p=0.214, 측모사진  p=0.584, 
측방두부규격방사선사진  p=0.827)(Figure 1 2). 모든 통계 분석은  Python  프로그램을  
사용하여  수행되었으며 , p값이 0.05 미만인  경우 통계적으로  유의미한  것으로  간
주하였다 . 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
- 42 - 
 Table 12. Comparison of accuracy for the classification between students and residents with 
and without the assistance of DCNN  
Accuracy (%)  Student  Resident  
without AI  with AI  without AI  with AI  
Frontal facial images  55.81  76.13 68.00 76.50 
Lateral facial images  54.84 72.42 62.33 74.50 
Lateral cephalometric 
radiographs  55.48  67.42 67.67 70.17  
 
 
Table 13. Comparison of accuracy for the classification between prosthodontic and non -
prosthodontic residents with and without the assistance of DCNN   
Accuracy (%)  Prosthodontic resident  Non-prosthodontic resident  
without AI  with AI  without AI  with AI  
Frontal facial images  70.91  76.36 66.32 76.58 
Lateral facial images  64.55 78.18 61.05 72.37 
Lateral cephalometric 
radiographs  69.09 72.27 66.84 68.95 
 
 
 
 
 
 
 
 
- 43 - 
 Figure 1 1. Comparison of averag e accuracy of dental school students and residents with and 
without the assistance of DCNN  (*: p<0.05)  
 
 
Figure 12 . Comparison of average accuracy of prosthodontic residents and non -prosthodontic 
residents with and without the assistance of DCNN (*: p<0.05)  
 
 

- 44 - 
 본 설문조사에서는  딥러닝  프로그램의  정확도에  따라 딥러닝  프로그램이  오분
류한 사례도  함께 제시하였다 . 그 결과, 딥러닝  프로그램이  정확히  분류한  문항
에서는  모든 사례에서  분류 정확도가  동일하거나  상승하였으며 , 정확도가  하락한  
문항은  없었다 . 반대로 , 딥러닝  프로그램이  오분류한  문항에서는  전부 분류 정확
도가 하락하였다 (Figure 13 -18). 
Willis 비율별로  출제된  문항의 정답률을  비교한  결과, 학생과  전공의  모두 
Willis 비율이  0.91, 0.9  구간에서  분류 정확도가  가장 낮은 경향이  존재하였다 . 반
면, Willis 비율이  0.89 이하인  문제에서는  정모사진과  측모사진에서  평균 이하로  
분류하는  응답 비율이  뚜렷하게  증가하였다 . 측방두부규격방사선사진에서는  
Willis 비율이  0.88까지도  평균 이하임을  분류하지  못하는  경향이  존재하였다 . 
Willis 비율이  0.92 이상인  경우 학생과  전공의  모두 정상으로  분류하는  응답 비
율이 뚜렷하게  증가하였다 (Figure 13 -18). 
 
 
 
 
 
 
 
 
 
 
- 45 - 
 Figure 13. Comparison of correct response rates with and without AI assistance across Willis 
ratio ranges using frontal facial photos in dental students  
 
 
 
Figure 14 . Comparison of correct response rates with and without AI assistance across Willis 
ratio ranges using frontal facial photos in dental residents  
 

- 46 - 
 Figure 15 . Comparison of correct response rates with and without AI assistance across Willis 
ratio ranges us ing lateral  facial photos in dental students  
 
 
 
Figure 16 . Comparison of correct response rates with and without AI assistance across Willis 
ratio ranges using lateral  facial photos in dental residents  
 

- 47 - 
 Figure 17 . Comparison of correct response rates with and without AI assistance across Willis 
ratio ranges using lateral  cephalometric radiograph  in dental students  
 
 
 
Figure 18 . Comparison of correct response rates with and without AI assistance across Willis 
ratio ranges using lateral  cephalometric radiograph  in dental residents  
 

- 48 - 
 IV. 고찰  
 
 광범위한  보철 수복이  포함된  전악 보철 치료에서  수직 고경의  정확한  분석과  
설정은  치료의  시작점이며 , 기능적 , 심미적  성공을  좌우하는  중요한  요소이다 (65). 
수직 고경의  감소를  진단하기  위해 임상에서  다양한  방법이  사용되고  있으나 , 각 
방법은  모두 술자의  숙련도 , 환자의  협조도 , 측정 환경 등에 영향을  받아 반복 
측정 시 일관성이  떨어질  수 있다 (66),(67). 이에 따라 본 연구에서는  보다 객관
적이고  재현 가능한  방법으로써  표준화된  안면 사진 기반의  딥러닝  모델을  활용
하여 Willis의 방법을  적용하 였다. Willis의 방법은  수직 고경을  평가하는  대표적
인 안면계측법으로 , 동공에서  구열까지의  거리와  비저부에서  턱끝까지의  거리가  
유사하다는  안면 비율의  원리를  기반으로  하는 간단한  방법이다 . Willis 비율은  
서양인을  기준으로  제시되었으므로 (12) 본 연구에서는  한국인을  기준으로  하는 
기준값을  설정하기  위해 한국인을  대상으로  한 이전의  인체 계측 논문을  참고하
였다 (54),(55),(56),(57). 이전 연구들의  Willis 비율의  평균값은  총 0.96 ± 0.02 로 계
산되었으며 , 본 연구에서는  임상적으로  보수적인  기준값을  설정하기  위해 분류의  
기준값을  0.92로 설정하였다 . 
Willis의 안면계측법은  임상적으로  빠르고  직관적으로  적용할  수 있다는  장점
이 있으나 , 연조직  기준점  사용으로  인해 측정 시마다  연조직  압박 정도, 관찰자  
간 변동성에  따라 측정값이  달라질  수 있다 (21). 이를 보완하 기 위해 표준화된  
안면 디지털  사진을  이용하는  사진측량법 (photogrammetry) 을 활용할  수 있다. 사
진측량법을  이용하면  기구에  의한 연조직  접촉 없이 계측이  가능하며 , 안면계측
을 직접 시행한  값과 디지털로  계측한  값 간에 유의한  차이가  없는 것으로  보고
- 49 - 
 되었다 (23),(24),(25),(26),(27). 디지털  사진측량법은  촬영  시에만  환자의  협조가  필
요하고 , 환자에게  직접  계측  시 발생하는  연조직  압박이  존재하지  않으며 , 촬영
된 이미지의  영구  보관  및 반복  측정이  가능하다는  장점이  있다 (24). 따라서  본 
연구에서는  표준화된  안면 사진을  기반으로  딥러닝  모델이  Willis 비율을  분류하
도록 설계하였다 . 
 최근 머신러닝  및 딥러닝  기술이  치과 임상에  도입되면서  진단, 치료계획  수
립, 예후 예측의  정확도와  효율성이  크게 향상되고  있다 (68). 하지만  딥러닝  모델
을 개발하여  임상에  적용하기  위해서는  데이터  전처리 , 특성  추출 , 알고리즘  선
정, 하이퍼파라미터  조정  등의  전문  지식이  요구되며 , 이는  데이터  과학이나  프
로그래밍  지식이  부족한  임상의에게  높은  진입  장벽으로  작용한다 (69). 이러한  
한계를  극복하기  위해  최근에는  딥러닝  파이프라인 에서  하나  이상의  단계를  자
동으로  최적화할  수 있는  자동화된  딥러닝  기술이  개발되었다 . 이러한  기성품  형
태의  소프트웨어나  플랫폼은  딥러닝에  대한  전문  지식  없이도  사용할  수 있으며 , 
단순한  추론  구조를  통해  임상  환경에  쉽게  적용이  가능하다 (40). 본 연구에서는  
학습  및 추론  과정에서  효과적인  딥러닝  모델을  자동으로  생성하고  하이퍼파라
미터를  최적화하는  Neuro -T 프로그램을  사용하였다 .  
본 연구에서  Willis 비율 분류는  안면 랜드마크  간 거리의  비율을  기준으로  하
며, 안면 이미지에서  해부학적  좌표를  추출하 는 연구들이  선행되어  왔다 (70),(71). 
하지만  대부분의 딥러닝  기반 랜드마크  추출 방법은  좌표 회귀, 후처리  등 여러 
단계가  결합된  복잡한  파이프라인을  필요로  한다 (72). 반면 Neuro -T는 이미지  전
체의 패턴을  end-to-end로 학습하여  별도의  좌표 추출이나  복잡한  후처리  없이 직
접적으로  이미지  분류를  시행하며 , 본 연구에서는  1,400장의  크롭  정모사진에서  
- 50 - 
 Willis 비율을  정상  그룹과  평균  이하  그룹으로  분류하는  정확도가  Neuro -T 프로
그램에서  90.00% 의 정확도 , 90.00% 의 정밀도 , 90.00% 의 재현율 , 90.00% 의 F1 점수
를 달성하였다 . 이러한  결과는  기존  연구 (73),(74)에서  보고된  Neuro -T의 분류  성
능과  유사한  수준으로 , 5,162개의  내시경  영상을  이용하여  식도암 , 전구병변 , 비종
양 등을  진단하는  선행  연구(73)에서  Neuro -T 프로그램은  95.6% 의 정확도 , 78.0%
의 정밀도 , 93.9% 의 재현율 , 85.2% 의 F1 점수를  달성했다 . 또한  2,899개의  내시경  
영상을  이용하여  위암의  침윤  깊이를  점막에  국한된  병변과  점막하  침윤  병변으
로 분류하는  선행  연구 (74)에서  Neuro -T 프로그램은  89.3% 의 정확도 , 89.1% 의 정
밀도 , 88.4% 의 재현율 , 88.7% 의 F1 점수를  달성했다 . 따라서  Neuro -T는 안면  이미
지를  기반으로  한 Willis 비율  분류에도  충분히  적용  가능한  가능성을  보여준다 . 
한편, 딥러닝  모델의  성능에  영향을  미치는  주요 요인 중 하나는  학습 데이터
의 클래스  간 불균형이다 . 특히 의료 영상 데이터는  개인정보  보호, 희귀 질환의  
낮은 발생률 , 라벨링에  필요한  전문성  등의 제약으로  인해 대규모  데이터셋  확보
가 어렵고 , 이는 과적합 (overfitting) 과 편향 (bias) 문제로  이어질  수 있다 (75). 실제
로 본 연구에서 도 크롭 정모사진에서  불균형한  데이터  세트를  적용하여  Neuro -T 
프로그램을  훈련시켰 을 때 무작위  언더샘플링 을 통해 균형을  맞춘 경우보다  재
현율과  F1 점수가  상대적으로  낮았으며 , 이는 소수 그룹에  대한 분류 성능이  저
하된 것을 의미한다 . 불균형한  데이터  세트를  균형있게  맞추는  것은 모델 성능, 
일반화 , 소수 그룹의  신뢰도  향상에  필수적이다 (76). 본 연구에서는  무작위  언더
샘플링을  통해 다수 클래스인  정상 그룹에서  데이터를  임의로  제거하여 , 소수 클
래스인  평균 이하 그룹과의  데이터  균형을  맞추어  클래스  불균형에  따른 편향을  
완화하였다 (77). 다만 이 과정에서  정보의  무작위  손실로  데이터의  대표성이  저
- 51 - 
 하될 위험이  있다는  단점이  존재한다 (78). 그럼에도  본 연구에서는  언더샘플링  
후 딥러닝  모델의  성능이  향상되어 , 학습 데이터의  균형을  맞추는  것이 유의미한  
전략임을  확인하였다 . 하지만  Willis 비율이  평균 이하인  그룹의  샘플 수를 증가
시키기  위해 추후 다기관  협력을  통한 확장된  데이터  수집이  필요하며 , 이를 통
해 모델의  일반화  성능을  더욱 향상시킬  수 있을 것이다 . 
또한, 딥러닝  모델의  분류 성능은  입력 이미지의  구성과  정보의  밀도에  따라서
도 영향을  받을 수 있다. 본 연구에서 는 크롭 사진에서  Willis 비율을  분류하는  
Neuro -T 프로그램의  정확도가  표준 사진에서보다  더 높게 나타났다 . 이는 관심 
영역을  축소시키면  딥러닝  모델의  정확도가  높아진다는  기존의  연구 결과와  일
치한다 (79),(80). 예를  들어 , 파노라마  영상에서  골다공증을  진단하는  이전의  딥러
닝 연구에서 는(79) 하악 경계의  좌측과  우측의 중간 영역으로 관심영역을  제한하
였을 때, Grad -CAM  상의 빨간색  관심영역이  연구자의  관심영역에  더욱 일치하는  
경향을  보이며  골다공증  선별 성능이  향상되었다고  보고하였다 . 본 연구에서도  
입력 이미지의  관심 영역이  딥러닝의  성능에  미치는  영향을  시각적으로  확인하
기 위해 Neuro -T 프로그램이  정확하게  분류한 사진과  오분류 한 사진의  Grad -
CAM을 분석하였다 . 분석 결과, 표준 사진에서는  오분류한  사진 뿐 아니라  정확
하게 판단한  사진에서도  빨간색  관심영역이  안면의  바깥에  위치하거 나 넓게 퍼
진 사례가  다수 관찰되었으며 , 특히 측방두부규격 방사선사진의  경우 두경부  골격 
구조로도  빨간색  관심영역이  분산되는  경향이  관찰되었다 . 반면, 크롭사진의  경
우 정확하게  판단한  사진에서는  주요 안면 부위에  빨간색  관심영역이  설정되었
다(Figure 5) . 따라서  DCNN  기반 프로그램이  Willis 비율을  보다 정확히  분류하기  
위해서는  학습 시 주요 안면 부위를  특정하는  것이 중요한  전략이  될 수 있다. 
- 52 - 
 본 연구에서는  수동으로  이미지를  크롭하여  관심영역을  제한하였으며 , 향후에는  
안면 주요 부위를  자동으로  탐지하여  크롭하는  딥러닝  기반 관심영역  탐색 기법
의 성능을  평가하는  연구가  필요할  것이다 . 
Willis 비율 분류를  위한 본 연구에서는  정모사진 , 측모사진 , 측방두부규격방사
선사진이라는  세 가지 안면 이미지  유형을  비교하였으며 , 분류 정확도는  정모사
진을 사용한  모델에서 가장 높았으며 , 그 뒤를 측모사진이  따랐으며 , 측방두부규
격방사선사진 에서 가장 낮은 정확도를  보였다 . 이는 정모사진에서는  양측 눈, 코, 
입 등 핵심 구조가  모두 드러나기  때문에  딥러닝  모델이  더 많은 정보를  학습할  
수 있기 때문으로  해석된다 (81). 실제로  기존 연구에서도  얼굴 인식 알고리즘은  
정모사진에  대해서는  우수한  성능을  보이지만 , 측모사진에  대해서는  상대적으로  
정확도가  저하된다고  보고된  바 있다 (82),(83). 측모사진의  경우 안면의  일부 구
조만이  노출되기  때문에 , 모델이  학습 가능한  해부학적  정보가  제한될  수 있으며
(83), 이로 인해 정면 사진보다  분류 정확도가  낮게 나타났을  수 있다. 한편, 측
방두부규격 방사선사진에서  딥러닝은  해부학적  구조의  중첩, 화질 저하, 보철물 , 
교정장치  등 인공물에  의한 영향 등으로  인해 여전히  한계가  있다 (70). 따라서  
이러한  방사선  영상의  한계를  보완하기  위해, 측방두부규격방사선사진을  촬영하
지 않고 측모사진만을  이용하여  두부 계측 지표를  예측하고  방사선  영상의  정보
를 대체하려는  시도가  보고되고  있으며 , 이를 통해 임상 사진만으로  방사선  사진
에 준하는  진단 정보를  얻을 수 있어 방사선사진을  대체하거나  최소화할  수 있
는 가능성을  시사한다 (84),(85),(86). 실제로  본 연구에서  측방두부규격 방사선사진
에서 Willis 비율 분류의  정확도가  정모사진 , 측모사진에  비해 가장 낮았으며 , 
Grad -CAM 분석 결과 주요 연조직  구조 뿐 아니라  두경부  골격 구조로  관심영역
- 53 - 
 이 분산되는  경향이  관찰되었다 . 이러한  결과는  모델이  주요 연조직  안면 정보에  
집중하지  못하고 , 상대적으로  불필요한  영역까지  포함하여  학습되었을  가능성을  
시사한다 . 따라서  Willis 비율 분류에  있어서는  정모사진과  측모사진 이 더 효과적
인 이미지  유형으로  작용하였을  것으로  판단된다 . 다만, 측방두부규격 방사선사진
의 경우 정모사진과  측모사진에  비해 상대적으로  적은 데이터  수가 모델의  일반
화 성능 저하 가능성도  배제할  수 없을 것이다 .  
DCNN의 결과를  해석하는  방법은  모델이  분류  결과를  결정하는  데 활용한  주
요 특징과  정보가  집중된  이미지  영역을  시각적  히트맵으로  표현하는  것이다  
(87). 이러한  방법  중 Grad-CAM이 대표적인  방법으로 , Grad -CAM은 DCNN  프로
그램의  가장 마지막  컨볼루션  레이어의  출력을  활용하여  클래스  활성화  맵(class 
activation map) 을 생성한다 . 클래스  활성화  맵에서  주황색과  빨간색으로  표시된  
영역이  딥러닝  모델이  주로 참고한  부분으로 , 본 연구에서는  Grad -CAM을 활용
하여 딥러닝  모델이  Willis 비율을 잘못 분류한  사례에서  집중한  관심영역을  시
각적으로  평가하였다 . 그 결과, 오분류  사례에서  세 가지 주요 패턴을  확인하였
다.  
 첫째, Willis 비율 분류 경계에  위치한  사진을  잘못된  그룹으로  판단한  경우가  
있었다 . 크롭 정모사진에서 는 전체 오분류  중 9.52%, 크롭 측모사진에서는  8.3%, 
크롭 측방두부규격방사선사진에서는  14.3% 가 이 유형에  해당하였다 . 이러한  사례
는 실제 비율이  임계값  근처에  위치하여  두 범주 간 형태 차이가  모호한  경우로 , 
모델이  일관된  판단을  내리기  어려웠던  것으로  해석된다 . 따라서  향후 연구에서
는 분류 경계에  위치한  이미지를  보다 충분히  포함할  수 있도록  데이터를  보완
할 필요가  있을 것이다 . 또한 Willis 비율을  기준으로  정상 그룹, 경계 그룹, 평균 
- 54 - 
 이하 그룹의  세 그룹으로  분류하여  딥러닝  프로그램을  훈련시킨다면  경계 사례
에 대한 예측의  명확성을  높이고 , 모델의  판단 기준을  보다 세분화함으로써  혼동
을 줄일 수 있을 것으로  기대된다 . 
 둘째 , Willis  비율 분류에  핵심적인  시각적  특징을  정확하게  인식하지  못하고 , 중
요하지  않은 영역에  주목하여  잘못 분류한  사례가  가장 많았다 . 크롭 정모사진에
서는 전체 오분류  중 90.48%, 크롭 측모사진에서는  70.8%,  크롭 측방두부규격방
사선사진에서는  78.6% 가 이 유형에  해당하였다 . 셋째, 모델이  과도하게  넓은 영
역에 주목하면서  판단의  기준이  분산되어  예측 오류로  이어진  사례가  일부 존재
하였다 . 이 유형은  크롭 정모사진에서는  관찰되지  않았고 , 크롭 측모사진에서  전
체 오분류  중 20.8%,  크롭 측방두부규격방사선사진에서는  7.1%가 해당하였다 . 둘
째, 셋째 유형 모두 모델이  분류에  필요한  핵심 특징을  명확히  식별하지  못하여  
발생하였고 , 결과적으로  잘못된  관심영역에  기반한  판단으로  이어졌다 . 이러한  
현상은  학습 데이터 에 노이즈 , 조명 등 불필요한  정보가  포함되어  있어 주변 정
보에 의존했기  때문일  수 있다. 특히 이 두 유형의  빈도는  Willis 비율이  경계에  
위치하여  오분류한  사례보다  더 높은 비율로  관찰되었으며 , 이는 모델의  시각적  
특징 인식 오류가  주요 오분류  원인임을  시사한다 . 따라서  Willis 비율을  분류하
는 딥러닝  모델의  성능 향상을  위해 모델이  집중해야  할 핵심 영역을  보다 명확
히 인식할  수 있도록  각 클래스의  대표적인  시각적  특징이  잘 드러나는  이미지
를 충분히  확보하고 , 경계 사례에  대한 추가 학습을  통해 분류의  안정성을  높일 
필요가  있다. 향후 연구에서는  이러한  개선 방향을  적용하여  딥러닝  모델의  정확
도를 높이고 , 임상적  적용 가능성을  향상시키기  위한 노력이  필요할  것이다 . 
본 연구에서 는 학습 샘플 수를 증가시켜  딥러닝  모델의  일반화  성능을  높이기  
- 55 - 
 위해 다양한  데이터  증강 기법을  적용하였다 . 특히 의료 영상 분야에서는  데이터
가 부족한  경우가  많기 때문에 , 기존 학습 데이터를  변형하여  추가적힌  학습 샘
플을 생성하는  데이터  증강은  모델의  성능을  향상시키는  핵심 기법으로 활용된
다(43). 그러나  본 연구에서는  해부학적  좌표 간 비율을  보존하기  위해 색상 기
반 증강(대비, 감마, 색조, 채도 변화), 좌우반전  증강, 경미한  회전(2° 시계방향 ) 
증강 등의 비교적  보존적인  증강 방식을  적용하였음에도 , 오히려  원본 이미지만
으로 학습한  모델이 가장 높은 Willis 비율 분류 정확도를  보였다 . 이는 다양한  
연구에서  데이터  증강이  딥러닝  기반 이미지  분류 성능을  유의하게  향상시킨다
는 보고 (45),(46),(47)와는  상반되는  결과이다 . 객체 인식을  위한 딥러닝  모델에서
는 색상 기반 증강, 좌우반전  증강, 회전 증강 방식 등을 시행했을  때 정확도가  
증가하는  경향이  있으나 (88), Willis 비율 분류와  같이 해부학적  비율에  기반한  분
류에서는  이러한  증강 기법이  오히려  주요 특징 학습에  방해가  될 수 있는 것으
로 사료된다 . 실제로  전통적인  데이터  증강 기법들을  별도의  수정 없이 의료 진
단  작업에  적용할  경우 오히려  부정적인  영향을  미친다는  보고가  있었다
(89),(90). 흉부 방사선  사진을  이용하여  폐렴이  의심되는지를  자동 분석하는  딥러
닝 선행 연구에서는  무작위  대비 조정이  모델의  성능이  향상되었고 , 무작위  회전 
기법의  경우 성능이  오히려  감소되었다고  보고하였다 (91). 특정 데이터  셋에 적
용한 데이터  증강 기법이  다른 데이터  셋에서도 일관된  성능 향상으로  이어지지
는 않으므로 (92), Willis 비율 분류에  가장 적합한  데이터  증강 기법의 종류, 강도, 
적용 방식에  대한 최적 조합을  찾는 후속 연구가  필요할  것으로  사료된다 .  
딥러닝  모델의  성능은  학습에  사용되는  데이터  샘플 수에 크게 영향을  받는다 . 
샘플 수가 너무 적을 경우 모델이  충분한  패턴을  학습하지  못해 과적합이나  편
- 56 - 
 향된 결과가  발생할  수 있으며 , 반대로  샘플 수가 지나치게  많을 경우 훈련 시간
이 과도하게  증가하고  연산 자원이 크게 소모될  수 있다(48),(93). 예를 들어 
CBCT  영상 기반의  치아 우식 진단에서  최적의  학습 샘플 수를 규명한  선행 연
구에서는  250개의 우식 사진에서  모델의  성능이  수렴했다고  보고하였다 (89).  
학습 곡선은  훈련 데이터  크기(x축)에 따라 모델의  성능 지표(y축)를 시각화하
여 딥러닝  모델의 학습 경향과  일반화  성능을  평가하는  데 유용한  도구이다 (94). 
학습 곡선을  분석하여  DCNN 이 최종 정확도에  도달하는  데 필요한  샘플 크기를  
계산할  수 있으며 , 본 연구에서는  학습 곡선 분석을  통해 그룹당  500장의 사진에
서 Willis 비율 분류 정확도가  뚜렷하게  수렴하는  경향을  관찰하였다 . 이는 안면 
비율 분류와  같은 특정 임상적  분류 과제에서 , 모델의  일반화  성능과  데이터  효
율성 간의 균형을  고려한  실질적  샘플 크기 설정에  참고가  될 수 있을 것으로  
생각된다 . 다만, 최적 샘플 크기는  데이터의  특성, 모델 구조, 분류 난이도  등에 
따라 달라질  수 있으므로 (49), 다양한  조건에서  반복적인  검증이  필요하 다. 
 본 연구에서는  Neuro -T 프로그램의  도움이  치과대학  학생과  전공의의  Willis 비
율 분류 정확도를  유의하게  향상시킴을  확인하였다 . 딥러닝  프로그램의  지원 전
후로 학생의  분류 정확도는  모든 유형의  사진에서  유의하게  향상되었고 , 전공의
의 분류 정확도는  정모사진 , 측모사진에서  유의하게  향상되었다 . 이러한  결과는  
딥러닝  기반 진단 보조 시스템이  학생, 전공의 , 전문가  모두의  진단 정확도를  유
의하게  향상시 킨다는  기존의  연구 결과와  일치한다 (95),(96). Neuro -T 프로그램의  
높은 분류 정확도가  치과대학  학생과  전공의  모두의  분류 정확도를  향상시키는  
데 기여하여  안모 비율 학습 및 임상 진단에  큰 도움이  될 수 있음을  시사한다 . 
다만 측방두부규격방사선사진에서  전공의의  분류 정확도가  유의하게  향상되지  
- 57 - 
 않았는데 , 이는 전공의의  딥러닝  지원 전 정확도가  67.67% 로 Neuro -T 프로그램의  
정확도 (73.07%) 와 비교적  근접한  수치를  기록하였으며 , 이로 인해 딥러닝  지원 
시 추가로  얻을 수 있는 정확도  향상의  여지가  상대적으로  적었던 것으로  판단
된다. 
Willis 비율 분류 시 임상 경험이  있는 전공의가  학생보다  더 높은 정확도를  
보일 것으로  예상하였다 . 실제로  전공의가  의과대학  학생보다  진단 정확도가  더 
높다는  것이 보고되어  있으며 (97), 본 연구에서 도 Neuro-T 프로그램  지원  전 전공
의의  분류  정확도가  학생의  분류  정확도보다  통계적으로  유의하게  높았다 . 또한 , 
의과대학  학생의  경우  전공의보다  딥러닝  프로그램의  지원  전후  진단  정확도  증
가량이  더 크다는  것이  보고되어  있으며(97), 본 연구에서 도 학생의  정확도  증가
량이  전공의보다  더 컸고 , 이로  인해  Neuro-T 프로그램  지원  후에는  두 집단  간
의 분류  정확도  차이가  통계적으로  유의하지  않게  되었다 . 이러한  결과는  딥러닝  
프로그램이  임상  경험에  상관없이  모든  참가자의  분류  정확도를  유사한  수준으
로 높여주며 , 이는  딥러닝  프로그램이  향후  교육용  도구로서  유용하다는  것의  중
요한  근거가  될 수 있다 . 
 또한 Willis 비율 분류 시 보철과  전공의가  보철과가  아닌 전공의보다  더 높은 
정확도를  보일 것으로  예상하였으나 , 본 연구에서는  딥러닝의  지원  전후  모두  보
철과  전공의와  보철과가  아닌  전공의의  분류  정확도  간에  통계적으로  유의한  차
이는  없었다 . 이는  안모를  분류하는  것이  특정  진료과에  국한된  특수한  전문성보
다는 , 일반적인  임상  경험에  기반한  진단  능력과  더 밀접하게  연관되어  있을  가
능성을  시사한다 . 특히  본 연구는  Willis 비율이라는  단일  지표에  기반한  정량적  
판단을  중심으로  하였기  때문에 , 보철과  전공의가  보유한  심미적  관찰력이나  안
- 58 - 
 모에  대한  종합적  분석  경험이  정확도에  반영되기  어려웠을  수 있다 .  
 본 설문조사에서는  딥러닝  프로그램의  판단이  평가자의  분류  정확도에  미치는  
영향을  분석하기  위해 , Neuro -T가 실제로  정답으로  분류한  문항과  오분류한  문항
을 정확도  비율에  맞추어  설문에  포함하였다 . 그 결과 , Neuro -T가 정답을  제시한  
문항에서는  평가자의  분류  정답률이  항상  유지되거나  향상된  반면 , 오답을  제시
한 경우에는  예외  없이  정답률이  하락하였다 . 이러한  결과는  평가자들이  딥러닝  
프로그램의  결과를  판단  근거로  적극적으로  받아들이고  있으며 , 딥러닝  프로그램
의 신뢰성과  정확성이  임상적  의사결정에  직접적인  영향을  미칠  수 있다는  점을  
시사한다 . 실제로  딥러닝  프로그램이  잘못된  판단을  내렸을  때 그를 참고하는  사
용자의  진단 정확도가  저하된 다는 이전 보고가  있었다 (98),(99). 따라서  임상 현
장 및 교육 환경 모두에서  딥러닝  프로그램의  신뢰도와  오류 가능성을  만드시  
고려해야  한다. 특히 교육 현장에서  AI를 활용할  때는 오분류  사례에  대한 명확
한 피드백과  설명을  제공하고 , AI 예측 결과의  신뢰도  정보를  함께 제시하여  학
습자가  비판적으로  정보를  수용할  수 있도록  유도하는  것이 필요할 것이다 (100). 
 Willis 비율  별 문항  분석에서는  0.91과 0.9 구간에서  정답률이  가장  낮게 나타
났으며 , 이 구간은  수치상으로 는 평균  이하에  속하지만 , 시각적으로 는 정상과  두
렷한  차이를  보이지  않기  때문에  임상적  판단에 서 혼란을  유발할  수 있는  영역
인 것으로  생각된다 . 반면 , Willis 비율이  0.89 이하로  낮아질수록  정모사진과  측
모사진에서  평균  이하로  분류하는  응답  비율이  증가하였으며 , 이는  일정  수준  이
상의  시각적  변화가  발생해야  평가자가  명확하게  평균  이하로  인식할  수 있음을  
나타낸다 . 하지만  측방두부규격방사선사진에서는  Willis 비율이  0.89 및 0.88인 문
항에서도  정답률이  낮았으며 , 이는  측방두부규격방사선사진이  다른  사진  유형에  
- 59 - 
 비해  Willis 비율  분류  난이도가  더 높다는  점을  시사한다 . Willis 비율이  0.92 이
상인  문항에서는  정상으로  분류하는  응답이  안정적으로  증가하였으며 , 이는  
Willis 비율이  정상  범위에  있는  경우  판단이  보다  일관되게  작용하고  있음을  보
여준다 . 
 다만 수직 고경은  하나의  지표만으로  판단하기  어려우며 , 여러 방법을  병행하여  
종합적으로  평가하는  것이 필수적이 다(101). Willis 비율만을 기반으로  한 분류는  
수직 고경의  감소 여부를  정밀하게  진단하기에는  한계가  있으며 , 이는 본 논문의  
제한점으로  작용할  수 있다. 향후 수직 고경의  감소 진단에  도움이  되는 추가적
인 임상 정보를  활용한  추가 연구가  필요할  것이다. 또한 본 연구는  후향적  연구
로 전남대학교  치과병원에서만  데이터를  수집하였기  때문에  스펙트럼  편향의  가
능성이  존재하며 , 이로 인해 결과의  직접적인  임상 적용에  한계가  있을 수 있다. 
실제로  원본 사진을  딥러닝  훈련에  사용하기  위한 표준화  과정이  필요하였다 . 이
러한 한계를  극복하기  위해서는  잘 설계된  전향적 연구를  통해 고품질의  대규모  
데이터  세트를  수집하는  것이 필요하다 . 특히 촬영 장치, 광학 구성 요소 등 사
진 획득 장비의  사양, 촬영 감도, 노출 조건, 조리개  값, 셔터 속도 및 환자의  자
세, 촬영 거리 등의 조건을  철저히  표준화하는  과정이  필요할  것이다 . 이러한  접
근은 딥러닝  프로그램의  일반화  가능성을  높이고 , 다양한  임상 환경에서의  적용 
가능성을  평가하는  데 도움이  될 것이다 . 또한 렌즈, 거리 등에 의한 왜곡이  최
소화된  3차원 영상의  데이터  적용, 그리고  Willis 비율을  분류하는  딥러닝  모델에 
더 다양한  증강 기법을  비교하는  연구가  추가적으로  필요하리라  사료된다 . 
 
 
- 60 - 
 V. 결론  
 
 본 연구에서  다음과  같은 결론을  얻었다 . 
1. Neuro -T 프로그램 을 이용한  Willis 비율 분류에서 , 표준 사진보다  주요 안면 
부위가  포함된  크롭 사진에서  정확도가  더 높았다 . 
2. Willis 비율 분류의  정확도는  정모사진에서  가장 높았고 , 측모사진 , 측방두부
규격방사선사진  순으로  낮아졌으며 , 이러한  경향은  표준 사진과  크롭 사진 
모두에서  일관되게  나타났다 . 
3. 크롭 정모사진을  활용한  모델에서는  데이터  증강 없이 원본 이미지만으로  훈
련했을  때, 데이터  증강을  적용한  모델보다  더 우수한  성능을  보였다 . 
4. Neuro -T 프로그램에서  Willis 비율을  정확히  분류하기  위한 최적의  크롭 정모
사진 개수는  그룹당  500장이었다 . 
5. Willis 비율 분류에서  Neuro -T 프로그램은  전공의보다  높은 정확도를  보였으
며, Neuro -T의 결과를  제공했을  때 치과대학  학생과  전공의의  분류 정확도 는 
유의하게  향상되었다 . 
 결론적으로 , 딥러닝  기반 Neuro -T 프로그램은  Willis 비율 분류에서  교육적 , 임상
적으로  유용한  도구가  될 수 있음을  확인하였다 . 본 연구 결과는  향후 임상 진단 
자동화  및 치의학  교육 지원에  기여할  수 있을 것으로  기대된다 . 
 
 
 
 
- 61 - 
 참고  문헌  
1. Edition N. The glossary of prosthodontic terms. The Journal of prosthetic dentistry. 
2017;117:e1 -e105.  
2. Burak N, Kaidonis J, Richards L, Townsend G. Experimental studies of human 
dentine wear. Archives of Oral Biology. 1999;44(10):885 -7. 
3. Crothers A, Sandham A. Vertical heigh t differences in subjects with severe dental 
wear. The European Journal of Orthodontics. 1993;15(6):519 -25. 
4. Doo-Yeon Hwang, Y ang J -H. Vertical Dimension : A Literature Review. The Journal 
of Korean Academy of Prosthodontics. 1997;35(1):211 -20. 
5. Turner  KA, Missirlian DM. Restoration of the extremely worn dentition. The Journal 
of prosthetic dentistry. 1984;52(4):467 -74. 
6. Abduo J, Lyons K. Clinical considerations for increasing occlusal vertical dimension: 
a review. Australian dental journal. 2012;57(1 ):2-10. 
7. Johansson A, Omar R. Identification and management of tooth wear. International 
Journal of Prosthodontics. 1994;7(6).  
8. Rivera -Morales WC, Mohl ND. Relationship of occlusal vertical dimension to the 
health of the masticatory system. The Journal  of prosthetic dentistry. 1991;65(4):547 -53. 
9. Weinberg LA. Vertical dimension: a research and clinical analysis. The Journal of 
Prosthetic Dentistry. 1982;47(3):290 -302. 
10. Pleasure MA. Correct vertical dimension and freeway space. The Journal of the 
American Dental Association. 1951;43(2):160 -3. 
11. Willis FM. Esthetics of full denture construction. The Journal of the American Dental 
Association. 1930;17(4):636 -42. 
12. Fayz F, Eslami A. Determination of occlusal vertical dimension: a literature review. 
The Journal of prosthetic dentistry. 1988;59(3):321 -3. 
- 62 - 
 13. Park J -H, Jeong C -M, Jeon Y -C, Lim J -S. A study on the occlusal plane and the 
vertical dimension in Korean adults with natural dentition. The Journal of Korean Academy 
of Prosthodontics. 2005:41 -51. 
14. Silverman MM. The speaking method in measuring vertical dimension. The Journal 
of Prosthetic Dentistry. 1953;3(2):193 -9. 
15. Shanahan TE. Physiologic vertical dimension and centric relation. Journal of 
Prosthetic Dentistry. 1956;6(6):741 -7. 
16. Lytle  RB. Vertical relation of occlusion by the patient's neuromuscular perception. 
The Journal of Prosthetic Dentistry. 1964;14(1):12 -21. 
17. V an Willigen J, Rashbass C, Melchior H. ‘Byte‐ryte’, an apparatus for the 
determination of the preferred vertical dime nsion of occlusion required for the construction of 
complete denture prosthesis. Journal of Oral Rehabilitation. 1985;12(1):23 -5. 
18. Goldstein G, Goodacre C, MacGregor K. Occlusal vertical dimension: best evidence 
consensus statement. Journal of Prosthodo ntics. 2021;30(S1):12 -9. 
19. Tan H -K, Hooper PM, Baergen CG. V ariability in the shape of maxillary vestibular 
impressions recorded with modeling plastic and a polyether impression material. International 
Journal of Prosthodontics. 1996;9(3).  
20. Zarb GA, H obkirk J, Eckert S, Jacob R. Prosthodontic treatment for edentulous 
patients: complete dentures and implant -supported prostheses: Elsevier Health Sciences; 2012.  
21. Morata C, Pizarro A, Gonzalez H, Frugone -Zambra R. A craniometry -based 
predictive model to  determine occlusal vertical dimension. The Journal of prosthetic dentistry. 
2020;123(4):611 -7. 
22. VL Gomes LG, CLM Correia, BL Lucas, PM Carvalho. Vertical Dimension of the 
Face Analyzed by Digital Photographs. European Journal of Esthetic Dentistry. 200 8;3(4).  
23. Mosier M, Barmak BA, Gómez -Polo M, Zandinejad A, Revilla -León M. Digital and 
- 63 - 
 analog vertical dimension measurements: A clinical observational study. International Journal 
of Prosthodontics. 2021;34(4).  
24. Strajnic L, Peric M, Zivkovic N, Lemic  AM, Vucinic N, Milicic B. Comparison of 
Face Anthropometry and Digital 2D -Face Photogrammetry as Methods for Predicting Vertical 
Dimension of Occlusion. Int J Prosthodont. 2024(3):282 -92. 
25. Negi G, Ponnada S, Aravind NKS, Chitra P. Photogrammetric Corre lation of Face 
with Frontal Radiographs and Direct Measurements. J Clin Diagn Res. 2017;11(5):ZC79 -
ZC83.  
26. Dindaroğlu F, Kutlu P, Duran GS, Görgülü S, Aslan E. Accuracy and reliability of 3D 
stereophotogrammetry: a comparison to direct anthropometry and 2D photogrammetry. The 
Angle Orthodontist. 2016;86(3):487 -94. 
27. Tanner JM, Weiner J. The reliability of the photogrammetric method of 
anthropometry, with a description of a miniature camera technique. American journal of 
physical anthropology. 1949;7(2): 145-86. 
28. Pirayesh Z, Hassanzadeh -Samani S, Farzan A, Rohban MH, Ghorbanimehr MS, 
Mohammad -Rahimi H, et al. A deep learning framework to scale linear facial measurements 
to actual size using horizontal visible iris diameter: a study on an Iranian populat ion. Scientific 
Reports. 2023;13(1):13755.  
29. Khalifa M, Albadawy M. AI in diagnostic imaging: Revolutionising accuracy and 
efficiency. Computer Methods and Programs in Biomedicine Update. 2024:100146.  
30. Luke AM, Rezallah NNF. Accuracy of artificial int elligence in caries detection: a 
systematic review and meta -analysis. Head & Face Medicine. 2025;21(1):24.  
31. Surdu A, Budala DG, Luchian I, Foia LG, Botnariu GE, Scutariu MM. Using AI in 
Optimizing Oral and Dental Diagnoses —A Narrative Review. Diagnostic s. 2024 ;14(24):2804 . 
32. LeCun Y , Bengio Y , Hinton G. Deep learning. nature. 2015;521(7553):436 -44. 
- 64 - 
 33. Lee J -G, Jun S, Cho Y -W, Lee H, Kim GB, Seo JB, et al. Deep learning in medical 
imaging: general overview. Korean journal of radiology. 2017;18(4):570 -84. 
34. Ezhov M, Gusarev M, Golitsyna M, Y ates JM, Kushnerev E, Tamimi D, et al. 
Clinically applicable artificial intelligence system for dental diagnosis with CBCT. Scientific 
reports. 2021;11(1):15006.  
35. Lee D -W, Kim S -Y , Jeong S -N, Lee J -H. Artificial intelligence in fractured dental 
implant detection and classification: evaluation using dataset from two dental hospitals. 
Diagnostics. 2021;11(2):233.  
36. Forte C, V oinea A, Chichirau M, Yeshmagambetova G, Albrecht LM, Erfurt C, et al. 
Deep learning for i dentification of acute illness and facial cues of illness. Frontiers in medicine. 
2021;8:661309.  
37. Su Z, Liang B, Shi F, Gelfond J, Šegalo S, Wang J, et al. Deep learning -based facial 
image analysis in medical research: a systematic review protocol. BMJ open. 
2021;11(11):e047549.  
38. Adityatama R, Putra A T. Image classification of human face shapes using 
convolutional neural network Xception architecture with transfer learning. Recursive Journal 
of Informatics. 2023;1(2):102 -9. 
39. Grd P, Tomičić I, Barči ć E. Transfer Learning with EfficientNetV2S for Automatic 
Face Shape Classification. Journal of Universal Computer Science (JUCS). 2024;30(2).  
40. Waring J, Lindvall C, Umeton R. Automated machine learning: Review of the state -
of-the-art and opportunities for healthcare. Artificial intelligence in medicine. 
2020;104:101822.  
41. Lee J -H, Kim Y -T, Lee J -B, Jeong S -N. A performance comparison between 
automated deep learning and dental professionals in classification of dental implant systems 
from dental imagin g: a multi -center study. Diagnostics. 2020;10(11):910.  
- 65 - 
 42. Mumuni A, Mumuni F. Data augmentation: A comprehensive survey of modern 
approaches. Array. 2022;16:100258.  
43. Li J, Pan J, Toh K -C, Zhou P. Towards Understanding Why Data Augmentation 
Improves Gen eralization. arXiv preprint arXiv:250208940. 2025.  
44. Shorten C, Khoshgoftaar TM. A survey on image data augmentation for deep learning. 
Journal of big data. 2019;6(1):1 -48. 
45. Kumar S, Asiamah P, Jolaoso O, Esiowu U. Enhancing Image Classification with 
Augmentation: Data Augmentation Techniques for Improved Image Classification. arXiv 
preprint arXiv:250218691. 2025.  
46. Perez L, Wang J. The effectiveness of data augmentation in image classification using 
deep learning. arXiv preprint arXiv:171204621. 201 7. 
47. Tang J, Sharma M, Zhang R. Explaining the Effect of Data Augmentation on Image 
Classification Tasks. Stanford University; 2020.  
48. Kim JR, Shim WH, Yoon HM, Hong SH, Lee JS, Cho YA, et al. Computerized bone 
age estimation using deep learning based program: evaluation of the accuracy and efficiency. 
American Journal of Roentgenology. 2017;209(6):1374 -80. 
49. Gulamali FF, Sawant AS, Kovatch P, Glicksberg B, Charney A, Nadkarni GN, et al. 
Autoencoders for sample size estimation for fully connected neur al network classifiers. npj 
Digital Medicine. 2022;5(1):180.  
50. Narayana PA, Coronado I, Sujit SJ, Wolinsky JS, Lublin FD, Gabr RE. Deep ‐
learning ‐based neural tissue segmentation of MRI in multiple sclerosis: effect of training set 
size. Journal of Magnet ic Resonance Imaging. 2020;51(5):1487 -96. 
51. Anderson PG, Tarder -Stoll H, Alpaslan M, Keathley N, Levin DL, Venkatesh S, et al. 
Deep learning improves physician accuracy in the comprehensive detection of abnormalities 
on chest X -rays. Scientific Reports. 2024;14(1):25151.  
- 66 - 
 52. Li J, Kot WY , McGrath CP, Chan BWA, Ho JWK, Zheng LW. Diagnostic accuracy 
of artificial intelligence assisted clinical imaging in the detection of oral potentially malignant 
disorders and oral cancer: A systematic review and meta -analysis. International Journal of 
Surgery. 2024;110(8):5034 -46. 
53. Lim YC, Abdul Shakor ASa, Shaharudin R. Reliability and accuracy of 2D 
photogrammetry: a comparison with direct measurement. Frontiers in Public Health. 
2022;9:813058.  
54. Sook -Hyun Park, Seong -Joo Jeo, Cho I -H. A Study on the Relationship of Between 
Facial and Oral Anatomic Landmark and Vertical Dimension in Korean Adults. A Journal of 
Korean Academy of Prosthodontics. 1992;30(1):43 -54. 
55. Jong-Hwan Park, Sang -Chun Oh, Dong J -K. A C omparative Study on the Several 
Facial Measurement Method for Vertical Dimension. A Journal of Korean Academy of 
Prosthodontics. 1995;33(1):75 -84. 
56. Ahn SK. A Study on the Vertical Dimension in Korean. 1967;12(11).  
57. Jeong S. The determination of verti cal dimension of Koreans using Willis gauge: 
Department of dentistry, School of dentistry, Seoul National University; 2011.  
58. Johnson JM, Khoshgoftaar TM. Survey on deep learning with class imbalance. 
Journal of big data. 2019;6(1):1 -54. 
59. Fotouhi S, A sadi S, Kattan MW. A comprehensive data level analysis for cancer 
diagnosis on imbalanced data. Journal of biomedical informatics. 2019;90:103089.  
60. Goutte C, Gaussier E, editors. A probabilistic interpretation of precision, recall and 
F-score, with impl ication for evaluation. European conference on information retrieval; 2005: 
Springer.  
61. Zhang Y , Hong D, McClement D, Oladosu O, Pridham G, Slaney G. Grad -CAM 
helps interpret the deep learning models trained to classify multiple sclerosis types using 
- 67 - 
 clinical brain magnetic resonance imaging. Journal of Neuroscience Methods. 
2021;353:109098.  
62. Chang -Hyun Park M -SP, Hyung -Suk Kim, Y un -Yeop Cha, Soon -Joong Kim, Youn -
Suk Ko, Min -Seok Oh, Eui -Hyoung Hwang, Byung -Cheul Shin, Chang -Eop Kim, Y un -Kyung 
Song. Th e Exploratory Analysis on the Registry Data of Patients with Low Back Pain 
Applying Correlation Analysis Method. Journal of Korean Medicine. 2017;27(4).  
63. Jurečková J, Picek J. Shapiro –Wilk -type test of normality under nuisance regression 
and scale. Comp utational Statistics & Data Analysis. 2007;51(10):5184 -91. 
64. Vierra A, Razzaq A, Andreadis A. Continuous variable analyses: T -test, Mann –
Whitney U, Wilcoxon sign rank.  Translational surgery: Elsevier; 2023. p. 165 -70. 
65. Minji Sun, Hong Seok Moon, Kim J. Evaluiation method of occlusal vertical 
dimension and their clinical applications: A narrative review. The Journal of Korean Academy 
of Prosthodontics. 2022;60(4):301 -12. 
66. Alhajj MN, Khalifa N, Abduo J, Amran AG, Ismail IA. Determination of occlusal 
vertical dimension for complete dentures patients: an updated review. Journal of oral 
rehabilitation. 2017;44(11):896 -907. 
67. Wong JY , Oh AK, Ohta E, Hunt A T, Rogers GF, Mulliken JB, et al. V alidity and 
reliability of craniofacial anthropometric measureme nt of 3D digital photogrammetric images. 
The Cleft Palate -Craniofacial Journal. 2008;45(3):232 -9. 
68. Ghaffari M, Zhu Y , Shrestha A. A review of advancements of artificial intelligence in 
dentistry. Dentistry Review. 2024:100081.  
69. Cofre -Martel S, Lopez Droguett E, Modarres M. Big machinery data preprocessing 
methodology for data -driven models in prognostics and health management. Sensors. 
2021;21(20):6841.  
70. Lee H, Cho JM, Ryu S, Ryu S, Chang E, Jung Y -S, et al. Automatic identification of 
- 68 - 
 posteroanter ior cephalometric landmarks using a novel deep learning algorithm: a comparative 
study with human experts. Scientific reports. 2023;13(1):15506.  
71. Kim I -H, Kim Y -G, Kim S, Park J -W, Kim N. Comparing intra -observer variation 
and external variations of a f ully automated cephalometric analysis with a cascade 
convolutional neural net. Scientific Reports. 2021;11(1):7925.  
72. Tang X, Guo F, Shen J, Du T. Facial landmark detection by semi -supervised deep 
learning. Neurocomputing. 2018;297:22 -32. 
73. Gong EJ, Ba ng CS, Jung K, Kim SJ, Kim JW, Seo SI, et al. Deep -learning for the 
diagnosis of esophageal cancers and precursor lesions in endoscopic images: A model 
establishment and nationwide multicenter performance verification study. Journal of 
Personalized Medicin e. 2022;12(7):1052.  
74. Bang CS, Lim H, Jeong HM, Hwang SH. Use of endoscopic images in the prediction 
of submucosal invasion of gastric neoplasms: automated deep learning model development 
and usability study. Journal of medical Internet research. 2021;23 (4):e25167.  
75. Schäfer R, Nicke T, Höfener H, Lange A, Merhof D, Feuerhake F, et al. Overcoming 
data scarcity in biomedical imaging with a foundational multi -task model. Nature 
Computational Science. 2024;4(7):495 -509. 
76. Buda M, Maki A, Mazurowski MA. A  systematic study of the class imbalance 
problem in convolutional neural networks. Neural networks. 2018;106:249 -59. 
77. Haixiang G, Yijing L, Shang J, Mingyun G, Y uanyue H, Bing G. Learning from class -
imbalanced data: Review of methods and applications. E xpert systems with applications. 
2017;73:220 -39. 
78. Oh J, Paik J. Clustering -based Undersampling for Imbalanced Data Classification. 
Journal of Korean Institute of Industrial Engineers. 2017:1910 -6. 
79. Lee K -S, Jung S -K, Ryu J -J, Shin S -W, Choi J. Evalua tion of transfer learning with 
- 69 - 
 deep convolutional neural networks for screening osteoporosis in dental panoramic 
radiographs. Journal of clinical medicine. 2020;9(2):392.  
80. Mishra BK, Thakker D, Mazumdar S, Neagu D, Gheorghe M, Simpson S. A novel 
application of deep learning with image cropping: a smart city use case for flood monitoring. 
Journal of Reliable Intelligent Environments. 2020;6:51 -61. 
81. Choi W, Ko H -S. Improved Face Frontalization Using Multiple Side -Face Images. 
Journal of the Korea  Computer Graphics Society. 2024;30(5):39 -46. 
82. Challapalli SSA, Bandireddi H, Pudi J. Profile Face Recognition and Classification 
Using Multi -Task Cascaded Convolutional Networks. Journal of Computer Allied Intelligence 
(JCAI, ISSN: 2584 -2676). 2024;2(6 ):65-78. 
83. Bezerra G, Gomes R. Recognition of occluded and lateral faces using mtcnn, dlib and 
homographies. Google Scholar. 2018:1 -4. 
84. Kartbak SBA, Özel MB, Kocakaya DNC, Çakmak M, Sinanoğlu EA. Classification 
of Intraoral Photographs with Deep Learn ing Algorithms Trained According to Cephalometric 
Measurements. Diagnostics. 2025;15(9):1059.  
85. Kocakaya DNC, Özel MB, Kartbak SBA, Çakmak M, Sinanoğlu EA. Profile 
Photograph Classification Performance of Deep Learning Algorithms Trained Using 
Cephalomet ric Measurements: A Preliminary Study. Diagnostics. 2024;14(17):1916.  
86. Takahashi K, Shimamura Y , Tachiki C, Nishii Y , Hagiwara M. Cephalometric 
landmark detection without X -rays combining coordinate regression and heatmap regression. 
Scientific reports.  2023;13(1):20011.  
87. Chien J -C, Lee J -D, Hu C -S, Wu C -T. The usefulness of gradient -weighted cam in 
assisting medical diagnoses. Applied Sciences. 2022;12(15):7748.  
88. Taylor L, Nitschke G, editors. Improving deep learning with generic data 
augmentation . 2018 IEEE symposium series on computational intelligence (SSCI); 2018: 
- 70 - 
 IEEE.  
89. Hussain Z, Gimenez F, Yi D, Rubin D, editors. Differential data augmentation 
techniques for medical imaging classification tasks. AMIA annual symposium proceedings; 
2018.  
90. Pattilachan TM, Demir U, Keles E, Jha D, Klatte D, Engels M, et al. A critical 
appraisal of data augmentation methods for imaging -based medical diagnosis applications. 
arXiv preprint arXiv:230102181. 2022.  
91. Jaszcz A. Impact of augmentation techniques on the classification of medical images. 
learning. 2022;9(10):11.  
92. Cho-I Moon YSB, Min Hyung Choi, Onseok Lee. Robust Psoriasis Severity 
Classification by using Data Augmentation. The Transactions of the Korean Institute of 
Electrical Engineers. 2022;71 (12):1841 -7. 
93. Fang Y , Wang J, Ou X, Ying H, Hu C, Zhang Z, et al. The impact of training sample 
size on deep learning -based organ auto -segmentation for head -and-neck patients. Physics in 
Medicine & Biology. 2021;66(18):185012.  
94. Faes L, Wagner SK, Fu DJ, Liu X, Korot E, Ledsam JR, et al. Automated deep 
learning design for medical image classification by health -care professionals with no coding 
experience: a feasibility study. The Lancet Digital Health. 2019;1(5):e232 -e42. 
95. Ba W, Wang S, Shang M, Zha ng Z, Wu H, Y u C, et al. Assessment of deep learning 
assistance for the pathological diagnosis of gastric cancer. Modern Pathology. 
2022;35(9):1262 -8. 
96. Sato Y , Takegami Y , Asamoto T, Ono Y , Hidetoshi T, Goto R, et al. Artificial 
intelligence improves th e accuracy of residents in the diagnosis of hip fractures: a multicenter 
study. BMC musculoskeletal disorders. 2021;22(1):407.  
97. Fussell DA, Tang CC, Sternhagen J, Marrey VV , Roman KM, Johnson J, et al. 
- 71 - 
 Artificial Intelligence Efficacy as a Function of T rainee Interpreter Proficiency: Lessons from 
a Randomized Controlled Trial. American Journal of Neuroradiology. 2024;45(11):1647 -54. 
98. Dratsch T, Chen X, Rezazade Mehrizi M, Kloeckner R, Mähringer -Kunz A, Püsken 
M, et al. Automation bias in mammography: the impact of artificial intelligence BI -RADS 
suggestions on reader performance. Radiology. 2023;307(4):e222176.  
99. Jabbour S, Fouhey D, Shepard S, V alley TS, Kazerooni EA, Banovic N, et al. 
Measuring the impact of AI in the diagnosis of hospitalized pati ents: a randomized clinical 
vignette survey study. Jama. 2023;330(23):2275 -84. 
100. Sriram A, Ramachandran K, Krishnamoorthy S. Artificial Intelligence in Medical 
Education: Transforming Learning and Practice. Cureus. 2025;17(3):e80852.  
101. Majeed MI, Afz al M, Kashif M. Determination of occlusal vertical dimension in a 
section of Pakistani population using craniofacial measurements. Journal of University 
Medical & Dental College. 2015;6(1):1 -5. 
 
 
 
 
 
 
 
 
 
 
 
- 72 - 
 Assessing the a ccuracy and educational utility  
of deep learning -based Neuro -T model 
for facial proportion a nalysis using Willis’ method  
 
Suhun Kim  
 
Department of Dental Science, School of Dentistry,  
Chonnam National University  
(Supervised by Professor Kwi -dug Yun)  
 
(Abstract)  
Purpose: The purpose of this study was to evaluate the performance of a deep learning 
model (Neuro -T) in classifying facial proportions according to Willis cephalometric analysis 
as either within the normal range or below average. The model's classification perform ance 
was compared across three different image types: frontal facial photographs, lateral facial 
photographs, and lateral cephalometric radiographs. Furthermore, the study compared 
classification accuracy between standardized images and cropped images that  included key 
facial features. The impact of data augmentation techniques —specifically adjustments in 
contrast, brightness, hue, and saturation —on model performance was analyzed. The optimal 
number of images required to achieve the model's final accuracy w as also determined. 
Lastly, this study assessed whether providing the output of the deep learning model to dental 
- 73 - 
 students and residents significantly improved their classification accuracy of facial 
proportions based on the Willis analysis.  
Materials and Methods: A total of 1,808 frontal facial photographs, 1,602 lateral facial 
photographs, and 1,086 lateral cephalometric radiographs were collected from Chonnam 
National University Dental Hospital. All images were standardized by rotating and cropping 
unnecessary backgrounds according to anatomical landmarks. Both the standardized images 
and cropped images were classified into two groups —normal (above average) and below 
average —based on the Willis cephalometric method and were used to train Neu ro-T to 
evaluate classification accuracy. Data augmentation was performed using changes in 
contrast, brightness, hue, and saturation, and the model was retrained to evaluate the effects 
of augmentation. To determine the optimal number of training images, m odels were trained 
with increments of 100 images per group up to 700, and classification perfo rmance was 
assessed. Finally, 31  dental students and 30 residents were asked to classify images into two 
groups using the Willis analysis before and after being p rovided with the output of Neuro -T, 
and their accuracy changes were compared.  
Results: The classification accuracies of Neuro -T using standardized images were 84.66% 
for frontal facial photographs, 79.33% for lateral facial photographs, and 6 3.46% for late ral 
cephalometric radiographs, with the highest accuracy observed in frontal facial images. The 
same trend was found with cropped images, with classification accuracies of 90.00%, 
84.00%, and 73.07%, respectively. In all image types, cropped images outperf ormed 
standardized images in classification accuracy. Notably, in cropped frontal facial images, 
models trained without data augmentation outperformed those trained with augmentation. 
Additionally, the classification accuracy plateaued when the number of t raining images 
reached 500 per group. After receiving Neuro -T’s results, both dental students and residents 
- 74 - 
 showed a statistically significant improvement in classification accuracy for frontal facial 
photographs and lateral cephalometric radiographs (p<0. 05). 
Conclusion: The use of Neuro -T for classifying facial proportions based on Willis 
cephalometric analysis demonstrated the highest performance when using frontal facial 
images. Across all image types, cropped images yielded better classification performance 
than standardized images. For cropped frontal facial images, models trained without data 
augmentation performed better than those trained with augmented data. The optimal number 
of training images for achieving maximum accuracy in cropped front al facial images was 
500 per group. After utilizing the Neuro -T model, both dental students and residents 
significantly improved in their classification accuracy, demonstrating the clinical 
applicability of deep convolutional neural networks (DCNNs) for Wi llis facial analysis and 
supporting their potential as effective educational tools.  
